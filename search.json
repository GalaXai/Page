[
  {
    "objectID": "My_pojects.html",
    "href": "My_pojects.html",
    "title": "Page",
    "section": "",
    "text": "fast.ai Practical Deep Learning for Coders 2022 Notes\n\n\n\n\n\n\n\nJupyter\n\n\nfastai\n\n\n\n\nMy notes, code and models from the fastai’s Practical Deep Learning for Coders 2022. Includes part 1 and 2\n\n\n\n\n\n\nMar 17, 2023\n\n\n\n\n\n\n  \n\n\n\n\nForest analytics\n\n\n\n\n\n\n\nAnalytics\n\n\nJupyter\n\n\n\n\nApp for tracking and visualizing proggres from Forest mobile app\n\n\n\n\n\n\nMar 7, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Projects/Forest/forest.html",
    "href": "Projects/Forest/forest.html",
    "title": "Forest analytics",
    "section": "",
    "text": "Input Data\n\n\n\n\n\n\n  \n    \n      \n      Start Time\n      End Time\n      Tag\n      Note\n      Tree Type\n      Is Success\n    \n  \n  \n    \n      0\n      Mon Dec 19 23:31:07 GMT+01:00 2022\n      Mon Dec 19 23:41:07 GMT+01:00 2022\n      Unset\n      NaN\n      Cedar\n      True\n    \n    \n      1\n      Wed Dec 21 02:27:55 GMT+01:00 2022\n      Wed Dec 21 03:27:20 GMT+01:00 2022\n      Unset\n      NaN\n      Cedar\n      False\n    \n    \n      2\n      Sun Feb 12 10:59:44 GMT+01:00 2023\n      Sun Feb 12 11:57:10 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n    \n    \n      3\n      Sun Feb 12 13:58:58 GMT+01:00 2023\n      Sun Feb 12 15:58:58 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n    \n    \n      4\n      Sun Feb 12 17:52:56 GMT+01:00 2023\n      Sun Feb 12 18:57:36 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n    \n  \n\n\n\n\nRemoving unsuccessfull study attempts\n\n\nCode\ndf = df[df['Is Success'] == True]\n# df = df[df['Tag'] != \"Unset\"] -> Tag u set / whether u studing or smth else / We assume u always use app for study\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      Start Time\n      End Time\n      Tag\n      Note\n      Tree Type\n      Is Success\n    \n  \n  \n    \n      0\n      Mon Dec 19 23:31:07 GMT+01:00 2022\n      Mon Dec 19 23:41:07 GMT+01:00 2022\n      Unset\n      NaN\n      Cedar\n      True\n    \n    \n      2\n      Sun Feb 12 10:59:44 GMT+01:00 2023\n      Sun Feb 12 11:57:10 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n    \n    \n      3\n      Sun Feb 12 13:58:58 GMT+01:00 2023\n      Sun Feb 12 15:58:58 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n    \n    \n      4\n      Sun Feb 12 17:52:56 GMT+01:00 2023\n      Sun Feb 12 18:57:36 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n    \n    \n      5\n      Sun Feb 12 20:53:34 GMT+01:00 2023\n      Sun Feb 12 22:53:34 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n    \n  \n\n\n\n\n\n\nTime colums\nLets create the function that calculates time difference between Start Time and End Time\nApply our function to the dataframe and create new column for calculated values\n\n\nCode\nfrom datetime import datetime\n\ndef calc_time_diff(start_time,end_time):\n    # Convert string dates to datetime objects\n    start_time = datetime.strptime(start_time, '%a %b %d %H:%M:%S %Z%z %Y')\n    end_time = datetime.strptime(end_time, '%a %b %d %H:%M:%S %Z%z %Y')\n\n    # Calculate time difference in Minutes\n    time_diff = (end_time - start_time).total_seconds()//60\n\n    return time_diff\n\ndf['Study Time'] = df.apply(lambda row: calc_time_diff(row['Start Time'],row['End Time']),axis=1)\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      Start Time\n      End Time\n      Tag\n      Note\n      Tree Type\n      Is Success\n      Study Time\n    \n  \n  \n    \n      0\n      Mon Dec 19 23:31:07 GMT+01:00 2022\n      Mon Dec 19 23:41:07 GMT+01:00 2022\n      Unset\n      NaN\n      Cedar\n      True\n      10.0\n    \n    \n      2\n      Sun Feb 12 10:59:44 GMT+01:00 2023\n      Sun Feb 12 11:57:10 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n      57.0\n    \n    \n      3\n      Sun Feb 12 13:58:58 GMT+01:00 2023\n      Sun Feb 12 15:58:58 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n      120.0\n    \n    \n      4\n      Sun Feb 12 17:52:56 GMT+01:00 2023\n      Sun Feb 12 18:57:36 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n      64.0\n    \n    \n      5\n      Sun Feb 12 20:53:34 GMT+01:00 2023\n      Sun Feb 12 22:53:34 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n      120.0\n    \n  \n\n\n\n\nWe want to group our Study Time by each day its gonna be hard with our current date format.\nSo we will create another column Study Date\n\n\nCode\ndef get_date(row):\n    return datetime.strptime(row,'%a %b %d %H:%M:%S %Z%z %Y').date()\n\ndf['Study Date'] = df.apply(lambda row: get_date(row['Start Time']),axis=1)\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      Start Time\n      End Time\n      Tag\n      Note\n      Tree Type\n      Is Success\n      Study Time\n      Study Date\n    \n  \n  \n    \n      0\n      Mon Dec 19 23:31:07 GMT+01:00 2022\n      Mon Dec 19 23:41:07 GMT+01:00 2022\n      Unset\n      NaN\n      Cedar\n      True\n      10.0\n      2022-12-19\n    \n    \n      2\n      Sun Feb 12 10:59:44 GMT+01:00 2023\n      Sun Feb 12 11:57:10 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n      57.0\n      2023-02-12\n    \n    \n      3\n      Sun Feb 12 13:58:58 GMT+01:00 2023\n      Sun Feb 12 15:58:58 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n      120.0\n      2023-02-12\n    \n    \n      4\n      Sun Feb 12 17:52:56 GMT+01:00 2023\n      Sun Feb 12 18:57:36 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n      64.0\n      2023-02-12\n    \n    \n      5\n      Sun Feb 12 20:53:34 GMT+01:00 2023\n      Sun Feb 12 22:53:34 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n      120.0\n      2023-02-12\n    \n  \n\n\n\n\n\n\nResult\n\n\nCode\ndf['Study Date'] = pd.to_datetime(df['Study Date'])\ndf.set_index('Study Date', inplace=True)\ndaily_study_time = df.resample('D')['Study Time'].sum()\n\n# Creating a list of dates from the start date to end date.\n\nstart_date = df.index.min()\nend_date = df.index.max()\ndate_range = pd.date_range(start=start_date, end=end_date, freq='D')\n\n# Create a 2D array to represent the grid of squares\nnum_weeks = int(np.ceil(len(date_range) / 7))\ngrid = np.zeros((num_weeks, 7))\n\n# Fill in the grid with study times\nfor i, date in enumerate(date_range):\n    if date in daily_study_time.index:\n        grid[i // 7, i % 7] = daily_study_time[date]\n\n# Define the color map for the squares\ncmap = ['rgb(235, 237, 240)', 'rgb(198, 228, 139)', 'rgb(123, 201, 111)', 'rgb(35, 154, 59)', 'rgb(25, 97, 39)']\nbounds = [0, 60, 120, 180, 300, np.inf]\n\n# Define data for heatmap\ndata = go.Heatmap(z=grid, colorscale=cmap, zmin=bounds[0], zmax=bounds[-1], colorbar=dict(title=\"Studied Time\"))\n\n# Define layout\nlayout = go.Layout(\n    title='Study Activity',\n    xaxis=dict(\n        title='Day of Week',\n        tickmode='array',\n        tickvals=np.arange(7),\n        ticktext=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'],\n    ),\n    yaxis=dict(\n        title='Month',\n        tickmode='array',\n        tickvals=np.arange(num_weeks),\n        ticktext=[date.strftime('%b %Y') for date in date_range[::7]],\n    ),\n)\n\n# Create and show figure\nfig = go.Figure(data=data, layout=layout)\n\n\n\n\nCode\nfig_b = go.Figure(data=[go.Bar(x=df['Study Time'].index, y=df['Study Time'].values ,marker = dict(color = \"#8cc914\"))])\nfig_b.update_layout(xaxis_title=\"Study Days\", yaxis_title=\"Study Time\")\n\n# add range selector\nfig_b.update_layout(xaxis=dict(rangeselector=dict(\n    buttons=list([\n        dict(count=7, label=\"1w\", step=\"day\", stepmode=\"backward\"),\n        dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n        dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n        dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n        dict(count=1, label=\"TEST\", step=\"day\", stepmode=\"todate\"),\n        dict(step=\"all\")\n    ])\n), rangeslider=dict(visible=True), type=\"date\"))\n\n\n\n\n\n                                                \n\n\n\n                                                \n\n\nThats all for now 👋"
  },
  {
    "objectID": "Projects/fastai22/part1/08-collaborative-filtering-deep-dive.html",
    "href": "Projects/fastai22/part1/08-collaborative-filtering-deep-dive.html",
    "title": "p1-08 Collaborative Filtering Deep Dive",
    "section": "",
    "text": "The dataset we’ll be using is called MovieLens. This dataset contains tens of millions of movie rankings, although we will just use a subset of 100k of them for our example.\n\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ML_100k)\n\nThe data is not separated by commas but by tabs we can parse \\t as a delimiter so we can read the data\n\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None,\n                      names=['user','movie','rating','timestamp'])\nratings.head()\n\n\n\n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n    \n    \n      1\n      186\n      302\n      3\n      891717742\n    \n    \n      2\n      22\n      377\n      1\n      878887116\n    \n    \n      3\n      244\n      51\n      2\n      880606923\n    \n    \n      4\n      166\n      346\n      1\n      886397596\n    \n  \n\n\n\n\nAlthough this has all the information we need, it is not a particularly helpful way to look at this data. Below table shows the same data cross-tabulated into a human-friendly form.\n\nThe empty cells in this table are the things that we would like our model to learn to fill in (predict). Those are the places where a user has not reviewed the movie yet, presumably because they have not watched it. For each user, we would like to figure out which of those movies they might be most likely to enjoy.\nWe can do that by representing a movie as a tensor of numbers.\nFor example we could represent the movie The Last Skywalker as …\n\nlast_skywalker = np.array([0.98,0.9,-0.9])\n\nWe are scoring 0.98 as very science-fiction movie, 0.9 very action movie, and -0.9 as not an old movie. We could represent a user who likes modern sci-fi action movies as:\n\nuser1 = np.array([0.9,0.8,-0.6])\n\nAnd now if we multiply user by a movie and then sum up the numbers, so we can see the score if it’s high we can predict that the user would like the movie, and if it’s low (negative) we predict that the user wouldn’t like the movie\n\n(user1*last_skywalker).sum()\n\n2.1420000000000003\n\n\n\ncasablanca = np.array([-0.99,-0.3,0.8])\n\n\n(user1*casablanca).sum()\n\n-1.611"
  },
  {
    "objectID": "Projects/fastai22/part1/08-collaborative-filtering-deep-dive.html#creating-the-dataloaders",
    "href": "Projects/fastai22/part1/08-collaborative-filtering-deep-dive.html#creating-the-dataloaders",
    "title": "p1-08 Collaborative Filtering Deep Dive",
    "section": "Creating the DataLoaders",
    "text": "Creating the DataLoaders\nWhen showing the data, we would rather see movie titles than their IDs. The table u.item contains the correspondence of IDs to titles:\n\nmovies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1',\n                     usecols=(0,1), names=('movie','title'), header=None)\nmovies.head()\n\n\n\n\n\n  \n    \n      \n      movie\n      title\n    \n  \n  \n    \n      0\n      1\n      Toy Story (1995)\n    \n    \n      1\n      2\n      GoldenEye (1995)\n    \n    \n      2\n      3\n      Four Rooms (1995)\n    \n    \n      3\n      4\n      Get Shorty (1995)\n    \n    \n      4\n      5\n      Copycat (1995)\n    \n  \n\n\n\n\nWe can merge this with our ratings table to get the user ratings by title as well.\n\nratings = ratings.merge(movies)\nratings.head()\n\n\n\n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n      title\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n      Kolya (1996)\n    \n    \n      1\n      63\n      242\n      3\n      875747190\n      Kolya (1996)\n    \n    \n      2\n      226\n      242\n      5\n      883888671\n      Kolya (1996)\n    \n    \n      3\n      154\n      242\n      3\n      879138235\n      Kolya (1996)\n    \n    \n      4\n      306\n      242\n      5\n      876503793\n      Kolya (1996)\n    \n  \n\n\n\n\nWe can build a DataLoaders object using fastai liblary. By default, it takes the first column as the user, and the second column for the item (Here it is our movie), and the third column for the ratings. We need to change the value of item_name in our case to use the titles instead of the IDs:\n\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n    \n  \n  \n    \n      0\n      92\n      Lawnmower Man 2: Beyond Cyberspace (1996)\n      1\n    \n    \n      1\n      16\n      Stand by Me (1986)\n      5\n    \n    \n      2\n      1\n      Quiz Show (1994)\n      4\n    \n    \n      3\n      686\n      Duck Soup (1933)\n      4\n    \n    \n      4\n      529\n      In & Out (1997)\n      4\n    \n    \n      5\n      838\n      Mask, The (1994)\n      4\n    \n    \n      6\n      680\n      Wrong Trousers, The (1993)\n      5\n    \n    \n      7\n      682\n      Sex, Lies, and Videotape (1989)\n      3\n    \n    \n      8\n      294\n      Ransom (1996)\n      4\n    \n    \n      9\n      906\n      Heat (1995)\n      4\n    \n  \n\n\n\n\ndls.classes['user'][:10],dls.classes['title'][:10]\n\n((#10) ['#na#',1,2,3,4,5,6,7,8,9],\n (#10) ['#na#',\"'Til There Was You (1997)\",'1-900 (1994)','101 Dalmatians (1996)','12 Angry Men (1957)','187 (1997)','2 Days in the Valley (1996)','20,000 Leagues Under the Sea (1954)','2001: A Space Odyssey (1968)','3 Ninjas: High Noon At Mega Mountain (1998)'])\n\n\nCreating latent factors for the user and the movie.\n\nn_users  = len(dls.classes['user'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\n\nuser_factors = torch.randn(n_users, n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\n\nTo calculate the result for a particular movie and user combination, we have to look up the index of the movie in our movie latent factor matrix and the index of the user in our user latent factor matrix; then we can do our dot product between the two latent factor vectors. But look up in an index is not an operation our deep learning models know how to do. They know how to do matrix products, and activation functions.\nFortunately, it turns out that we can represent look up in an index as a matrix product. The trick is to replace our indices with one-hot-encoded vectors. Here is an example of what happens if we multiply a vector by a one-hot-encoded vector representing the index 3:\n\none_hot_3 = one_hot(3, n_users).float()\n\n\nuser_factors.t() @ one_hot_3\n\ntensor([ 1.6976,  0.1139,  0.0346,  0.4224, -0.1760])\n\n\nIt gives us the same vector as the one at index 3 in the matrix:\n\nuser_factors[3]\n\ntensor([ 1.6976,  0.1139,  0.0346,  0.4224, -0.1760])"
  },
  {
    "objectID": "Projects/fastai22/part1/08-collaborative-filtering-deep-dive.html#collaborative-filtering-from-scratch",
    "href": "Projects/fastai22/part1/08-collaborative-filtering-deep-dive.html#collaborative-filtering-from-scratch",
    "title": "p1-08 Collaborative Filtering Deep Dive",
    "section": "Collaborative Filtering from Scratch",
    "text": "Collaborative Filtering from Scratch\nNote that creating a new PyTorch module requires inheriting from Module. The thing that we need to know to create a new PyTorch module is that when our module is called. PyTorch will call a method in our call called forward, and will pass along to that any parameters that are included in the call.\n\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        return (users * movies).sum(dim=1)\n\nNote that the input of the model is a tensor of shape batch_size x 2, where the first column (x[:, 0]) contains the user IDs and the second column (x[:, 1]) contains the movie IDs. As explained before, we use the embedding layers to represent our matrices of user and movie latent factors:\n\nx,y = dls.one_batch()\nx.shape, x[:5],\"User IDs\", x[:,0][:5],\"Movie IDs\", x[:,1][:5]\n\n(torch.Size([64, 2]),\n tensor([[ 807,  255],\n         [ 881,  581],\n         [  58, 1417],\n         [  94, 1029],\n         [ 514,  274]]),\n 'User IDs',\n tensor([807, 881,  58,  94, 514]),\n 'Movie IDs',\n tensor([ 255,  581, 1417, 1029,  274]))\n\n\nNow that we have defined our architecture, and created our parameters matrices, we need to create Learner.\n\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\nWe are now ready to train our model:\n\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.341441\n      1.300256\n      00:09\n    \n    \n      1\n      1.097734\n      1.077599\n      00:10\n    \n    \n      2\n      0.939855\n      0.981324\n      00:09\n    \n    \n      3\n      0.841633\n      0.878749\n      00:08\n    \n    \n      4\n      0.783042\n      0.865914\n      00:09\n    \n  \n\n\n\nThe first thing we can do to make our model a little bit better is to force prediction to be between 0 and 5. For this, we can use already know sigmoid. One thing we is that it’s better to have the range go a little bit over 5, since sigmoid never reaches maximum value, so we will used (0, 5.5)\n\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\n\nAnd let’s train again:\n\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.995753\n      0.993011\n      00:09\n    \n    \n      1\n      0.904139\n      0.900687\n      00:11\n    \n    \n      2\n      0.659721\n      0.867739\n      00:09\n    \n    \n      3\n      0.494031\n      0.873210\n      00:10\n    \n    \n      4\n      0.363530\n      0.878632\n      00:12\n    \n  \n\n\n\nLooks a lot better, but we can do even better. One obvious missing thing is that some users are just more positive or negative in their recommendations than others, and some movies are just better or worse than others. But in our calculations we do not have any way to encode either of these things.\n\nWe can add a single number for each user that we can add to our scores, and ditto for each movie, that will handle this missing piece very nicely. This single number is called bias.\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.user_bias = Embedding(n_users, 1)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.movie_bias = Embedding(n_movies, 1)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        res = (users * movies).sum(dim=1, keepdim=True)\n        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n        return sigmoid_range(res, *self.y_range)\n\nLets train our model:\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.939353\n      0.929019\n      00:11\n    \n    \n      1\n      0.820720\n      0.854279\n      00:09\n    \n    \n      2\n      0.601930\n      0.859065\n      00:13\n    \n    \n      3\n      0.392151\n      0.886714\n      00:10\n    \n    \n      4\n      0.280966\n      0.893484\n      00:09\n    \n  \n\n\n\nOur training_loss goes down, but our valid_loss goes up at the end, this is a clear indication of overfitting. In this case, there is no way to use data augmentation, so we will have to use another regularization technique. One approach that can be helpful is weight dacay.\n\nWeight Decay\nWeight decay, or L2 regularization, consist in adding to your loss function the sum of all the weights squared. Why do that? Because when we compute the gradients, it will add a contribution to them that will encourage the weights to be as small as possible.\nWhy would it prevent overfitting? The idea is that the larger the coefficients are, the sharper canyons we will have in the loss function. If we take the basic example of a y = a * x**2, the larger a is, the more narrow the parabola is.\n\nx = np.linspace(-2,2,100)\na_s = [1,2,5,10,50] \nys = [a * x**2 for a in a_s]\n_,ax = plt.subplots(figsize=(8,6))\nfor a,y in zip(a_s,ys): ax.plot(x,y, label=f'a={a}')\nax.set_ylim([0,5])\nax.legend();\n\n\n\n\nWeight decay or just wd is a parameter that controls that sum of squares we add to our loss. > loss_with_wd = loss + wd * parameters**2.sum()\nIn practice, though, it would be very inefficient (and maybe numerically unstable) to compute that big sum and add it to the loss. We can just calculate the derivative of parameters**2 which is equal to 2 * p, so adding the big sum to our loss is exactly the same as doing: > loss_with_wd = wd * 2 * parameters\nSince wd is a parameter that we choose, we can just make it twice as big, so we don’t even need *2 part.\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.928787\n      0.949326\n      00:09\n    \n    \n      1\n      0.864657\n      0.881492\n      00:09\n    \n    \n      2\n      0.749078\n      0.829044\n      00:09\n    \n    \n      3\n      0.578540\n      0.819058\n      00:09\n    \n    \n      4\n      0.481202\n      0.819370\n      00:09\n    \n  \n\n\n\nAnd as expected, we don’t overfit!\n\n\nCreating Our Own Embedding Module\nSo far, we’ve used Embedding without thinking about how it really works. Let’s re-create DotProductBias without using this class.\nWe have to be careful, since optimizers require that they can get all the parameters of a module from the module’s parameters method. However, this does not happen fully automatically. If we just add a tensor as an attribute to a Modeul, it will not include parameters.\n\nclass T(Module):\n    def __init__(self): self.a = torch.ones(3)\n\nL(T().parameters())\n\n(#0) []\n\n\nTo tell the Modeul that we need a tensor as a parameter, we need to wrap it in nn.Parameter class. This class doesn’t actually add any functionality (other than automatically calling requires_grad_. It’s only used as a “marker” to show what to include in parameters.\n\nclass T(Module):\n    def __init__(self): self.a = nn.Parameter(torch.ones(3))\n\nL(T().parameters())\n\n(#1) [Parameter containing:\ntensor([1., 1., 1.], requires_grad=True)]\n\n\nAll pytorch modules use nn.Parameters for any trainable parameters.\n\nclass T(Module):\n    def __init__(self): self.a = nn.Linear(1, 3, bias=False)\n\nt = T()\nL(t.parameters())\n\n(#1) [Parameter containing:\ntensor([[0.2044],\n        [0.8197],\n        [0.2877]], requires_grad=True)]\n\n\n\ntype(t.a.weight)\n\ntorch.nn.parameter.Parameter\n\n\nWe can create a tensor as a parameter, with random init.\n\ndef create_params(size):\n    return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))\n\n\ncreate_params(tensor([3]))\n\nParameter containing:\ntensor([ 0.0009,  0.0076, -0.0064], requires_grad=True)\n\n\nLet’s use this to create DotProductBias again, but without the Embedding.\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = create_params([n_users, n_factors])\n        self.user_bias = create_params([n_users])\n        self.movie_factors = create_params([n_movies, n_factors])\n        self.movie_bias = create_params([n_movies])\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors[x[:,0]]\n        movies = self.movie_factors[x[:,1]]\n        res = (users*movies).sum(dim=1)\n        res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n        return sigmoid_range(res, *self.y_range)\n\nAnd let’s train:\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.920867\n      0.952832\n      00:13\n    \n    \n      1\n      0.833844\n      0.863312\n      00:10\n    \n    \n      2\n      0.742645\n      0.830492\n      00:13\n    \n    \n      3\n      0.589977\n      0.823320\n      00:13\n    \n    \n      4\n      0.463834\n      0.822690\n      00:12\n    \n  \n\n\n\nOur Embeding works.\nNow, let’s take a look at what our model has learned."
  },
  {
    "objectID": "Projects/fastai22/part1/08-collaborative-filtering-deep-dive.html#interpreting-embeddings-and-biases",
    "href": "Projects/fastai22/part1/08-collaborative-filtering-deep-dive.html#interpreting-embeddings-and-biases",
    "title": "p1-08 Collaborative Filtering Deep Dive",
    "section": "Interpreting Embeddings and Biases",
    "text": "Interpreting Embeddings and Biases\nIt is interesting to see what parameters it has discovered. The easiest to interpret are the biases. Here are the movies withe lowest values in the bias vector.\n\nmovie_bias = learn.model.movie_bias.squeeze()\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Children of the Corn: The Gathering (1996)',\n 'Lawnmower Man 2: Beyond Cyberspace (1996)',\n 'Robocop 3 (1993)',\n 'Amityville II: The Possession (1982)',\n 'Jury Duty (1995)']\n\n\nLowest bias value means that, even when a user is very well matched to its latent factors, they still generally don’t like it.\nNow let’s look at the movies with the highest values\n\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Shawshank Redemption, The (1994)',\n 'Titanic (1997)',\n \"Schindler's List (1993)\",\n 'L.A. Confidential (1997)',\n 'Star Wars (1977)']\n\n\nIt is not quite so easy to directly interpret the embedding matrix. There are just too many factors for a human to look at. But there is a technique that can pull the most important underlying directions in such a matrix. called principal component analysisc PCA.\n\ng = ratings.groupby('title')['rating'].count()\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\ntop_idxs = tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])\nmovie_w = learn.model.movie_factors[top_idxs].cpu().detach()\nmovie_pca = movie_w.pca(3)\nfac0,fac1,fac2 = movie_pca.t()\nidxs = list(range(50))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n\n\n\n\nWe can see here that the model seems to have discovered some sort of concept.\n\nJeremy: No matter how many models I train, I never stop getting moved and surprised by how these randomly initialized bunches of numbers, trained with such simple mechanics, manage to discover things about my data all by themselves. It almost seems like cheating, that I can create code that does useful things without ever actually telling it how to do those things!\n\n\nUsing fastai.collab\nWe can create and train a collaborative filtering model using the exact strucutre shown earlier by using fastai’s collab_learner\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\n\n\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.918921\n      0.946220\n      00:11\n    \n    \n      1\n      0.877677\n      0.871620\n      00:11\n    \n    \n      2\n      0.749717\n      0.833303\n      00:12\n    \n    \n      3\n      0.576543\n      0.822094\n      00:10\n    \n    \n      4\n      0.510695\n      0.821848\n      00:09\n    \n  \n\n\n\nThe names of the layers can be seen by printing the model:\n\nlearn.model\n\nEmbeddingDotBias(\n  (u_weight): Embedding(944, 50)\n  (i_weight): Embedding(1665, 50)\n  (u_bias): Embedding(944, 1)\n  (i_bias): Embedding(1665, 1)\n)\n\n\nWe can use these to replicate any of the analyses we did in the previous section.\n\nmovie_bias = learn.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Shawshank Redemption, The (1994)',\n 'Titanic (1997)',\n 'Star Wars (1977)',\n \"Schindler's List (1993)\",\n 'Silence of the Lambs, The (1991)']\n\n\n\n\nEmbedding Distance\nOn a two-dimensional map we can calculate the distance between two coordinates using the sqrt(x^2+y^2). For a 50-dimensional embedding, we can do exactly the same thing, except that we add up the squares of all 50 of the coordinate distances.\nWe can use this to find the most similar movie to Silence of the Lambs.\n\nmovie_factors = learn.model.i_weight.weight\nidx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']\ndistances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])\nidx = distances.argsort(descending=True)[1]\ndls.classes['title'][idx]\n\n'Stand by Me (1986)'"
  },
  {
    "objectID": "Projects/fastai22/part1/08-collaborative-filtering-deep-dive.html#deep-learning-for-collaborative-filtering",
    "href": "Projects/fastai22/part1/08-collaborative-filtering-deep-dive.html#deep-learning-for-collaborative-filtering",
    "title": "p1-08 Collaborative Filtering Deep Dive",
    "section": "Deep Learning for Collaborative Filtering",
    "text": "Deep Learning for Collaborative Filtering\nfastai has a function get_emb_sz that return recommended sizes for embedding matrices for your data, based on heuristic that fast.ai has found tends to work well in practice.\n\nembs = get_emb_sz(dls)\nembs\n\n[(944, 74), (1665, 102)]\n\n\n\nclass CollabNN(Module):\n    def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):\n        self.user_factors = Embedding(*user_sz)\n        self.item_factors = Embedding(*item_sz)\n        self.layers = nn.Sequential(\n            nn.Linear(user_sz[1]+item_sz[1], n_act),\n            nn.ReLU(),\n            nn.Linear(n_act, 1))\n        self.y_range = y_range\n        \n    def forward(self, x):\n        embs = self.user_factors(x[:,0]),self.item_factors(x[:,1])\n        x = self.layers(torch.cat(embs, dim=1))\n        return sigmoid_range(x, *self.y_range)\n\n\nmodel = CollabNN(*embs)\n\nCollabNN creates our Embedding layers in the same way as previous classes in this chapter, except that we now use the embs sizes.\n\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.01)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.918357\n      0.946983\n      00:12\n    \n    \n      1\n      0.918140\n      0.914214\n      00:14\n    \n    \n      2\n      0.833999\n      0.871329\n      00:12\n    \n    \n      3\n      0.840610\n      0.859441\n      00:13\n    \n    \n      4\n      0.752391\n      0.859842\n      00:14\n    \n  \n\n\n\nfastai provides this model in fastai.collab if u pass use_nn = True in your call to collab_learner and lets you easily create more layers. For instance, here we’re creating two hidden layers,, of size 100 and 50, respectively.\n\nlearn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50])\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.984000\n      0.979401\n      00:16\n    \n    \n      1\n      0.960109\n      0.911486\n      00:15\n    \n    \n      2\n      0.901726\n      0.880157\n      00:15\n    \n    \n      3\n      0.840057\n      0.853600\n      00:13\n    \n    \n      4\n      0.769136\n      0.854030\n      00:14\n    \n  \n\n\n\nlearn.model is an object of type EmbeddingNN. Let’s look at the code for this class.\n\n@delegates(TabularModel)\nclass EmbeddingNN(TabularModel):\n    def __init__(self, emb_szs, layers, **kwargs):\n        super().__init__(emb_szs, layers=layers, n_cont=0, out_sz=1, **kwargs)\n\nThis class inherits from TabularModel, which is where it gets all its functionality from. In __init__ it calls the same method in TabularModel, passing n_cont=0 and out_sz=1; other than that, it only passes along whatever arguments it received.\n\nEnd sidebar\nAlthough the results of EmbeddingNN are a bit worse than the dot product approach (which shows the power of carefully constructing an architecture for a domain), it does allow us to do something very important: we can now directly incorporate other user and movie information, date and time information, or any other information that may be relevant to the recommendation. That’s exactly what TabularModel does. In fact, we’ve now seen that EmbeddingNN is just a TabularModel, with n_cont=0 and out_sz=1. So, we’d better spend some time learning about TabularModel, and how to use it to get great results! We’ll do that in the next chapter.\n\n\nFurther Research\n\nTake a look at all the differences between the Embedding version of DotProductBias and the create_params version, and try to understand why each of those changes is required. If you’re not sure, try reverting each change to see what happens. (NB: even the type of brackets used in forward has changed!)\nFind three other areas where collaborative filtering is being used, and find out what the pros and cons of this approach are in those areas.\nComplete this notebook using the full MovieLens dataset, and compare your results to online benchmarks. See if you can improve your accuracy. Look on the book’s website and the fast.ai forum for ideas. Note that there are more columns in the full dataset—see if you can use those too (the next chapter might give you ideas).\nCreate a model for MovieLens that works with cross-entropy loss, and compare it to the model in this chapter."
  },
  {
    "objectID": "Projects/fastai22/part1/07-how-random-forests-really-work.html",
    "href": "Projects/fastai22/part1/07-how-random-forests-really-work.html",
    "title": "p1-07 How random forests really work",
    "section": "",
    "text": "Additional notes about Random Forests Link"
  },
  {
    "objectID": "Projects/fastai22/part1/07-how-random-forests-really-work.html#data-preprocessing",
    "href": "Projects/fastai22/part1/07-how-random-forests-really-work.html#data-preprocessing",
    "title": "p1-07 How random forests really work",
    "section": "Data preprocessing",
    "text": "Data preprocessing\nWe’ll create DataFrames from the CSV files just like we did in the “linear model and neural net from scratch” notebook, and do much the same preprocessing.\n\nimport os\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n\nif iskaggle: path = Path('../input/titanic')\nelse:\n    import zipfile,kaggle\n    path = Path('titanic')\n    kaggle.api.competition_download_cli(str(path))\n    zipfile.ZipFile(f'{path}.zip').extractall(path)\n    !rm titanic.zip\n\n    \ndf = pd.read_csv(path/'train.csv')\ntst_df = pd.read_csv(path/'test.csv')\nmodes = df.mode().iloc[0]\n\nDownloading titanic.zip to /root/Page/nbs/Projects/fastai22/part1\n\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34.1k/34.1k [00:00<00:00, 1.60MB/s]\n\n\n\n\n\n\n\n\nOne difference with Random Forests however is that we don’t generally have to create dummy variables like we did for non-numeric columns in the linear models and neural network. Instead, we can just convert those fields to categorical variables, which internally in Pandas makes a list of all the unique values in the column, and replaces each value with a number. The number is just an index for looking up the value in the list of all unique values.\n\ndef proc_data(df):\n    df['Fare'] = df.Fare.fillna(0)\n    df.fillna(modes, inplace=True)\n    df['LogFare'] = np.log1p(df['Fare']) # log1p is basically log(n+1)\n    df['Embarked'] = pd.Categorical(df.Embarked)\n    df['Sex'] = pd.Categorical(df.Sex)\n\nproc_data(df)\nproc_data(tst_df)\n\nWe’ll make a list of the continuous, categorical, and dependent variables. Note that we no longer consider Pclass a categorical variable. That’s because it’s ordered (i.e 1st, 2nd, and 3rd class have an order), and decision trees, as we’ll see, only care about order, not about absolute value.\n\ncats=[\"Sex\",\"Embarked\"]\nconts=['Age', 'SibSp', 'Parch', 'LogFare',\"Pclass\"]\ndep=\"Survived\"\n\nEven although we’ve made the cats columns categorical, they are still shown by Pandas as their original values:\n\ndf.Sex.head()\n\n0      male\n1    female\n2    female\n3    female\n4      male\nName: Sex, dtype: category\nCategories (2, object): ['female', 'male']\n\n\nHowever behind the scenes they’re now stored as integers, with indices that are looked up in the Categories list shown in the output above. We can view the stored values by looking in the cat.codes attribute:\n\ndf.Sex.cat.codes.head()\n\n0    1\n1    0\n2    0\n3    0\n4    1\ndtype: int8"
  },
  {
    "objectID": "Projects/fastai22/part1/07-how-random-forests-really-work.html#binary-splits",
    "href": "Projects/fastai22/part1/07-how-random-forests-really-work.html#binary-splits",
    "title": "p1-07 How random forests really work",
    "section": "Binary splits",
    "text": "Binary splits\nRandom Forest is a ensemble of trees and a tree is a ensemble of binary splits. So lets start with binary splits\nA binary split is where all rows are placed into one of two groups (splits), based on whether they’re above or below certain threshold of some column. For example, we could split the rows of our dataset into males and females, by using the threshold 0.5 and the column Sex.\n\nimport seaborn as sns\n\nfig,axs = plt.subplots(1,2, figsize=(11,5))\nsns.barplot(data=df, y=dep, x=\"Sex\", ax=axs[0]).set(title=\"Survival rate\")\nsns.countplot(data=df, x=\"Sex\", ax=axs[1]).set(title=\"Histogram\");\n\n\n\n\nWe can see on the left graph that if we split the date into males and females, we’d have groups that have very different survival rates: >70% for females, and <20% for males. We can also see on the right graph that the split would be reasonably even, with 300 passengers (out of around 900) in each group.\nFurthermore, we could create a very simple “model” which would say that all female survive, and no males do. To do so, we better first split our data into a training and validation set.\n\nfrom numpy import random\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(42)\ntrn_df,val_df = train_test_split(df, test_size=0.25)\ntrn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes)\nval_df[cats] = val_df[cats].apply(lambda x: x.cat.codes)\n\n(In the previous step we also replaced the categorical variables with their integer codes, since some of the models we’ll be building in a moment require that.)\nNow we can create our independent variables (the x variables) and dependent (the y variable):\n\ndef xs_y(df):\n    xs = df[cats+conts].copy()\n    return xs,df[dep] if dep in df else None\n\ntrn_xs,trn_y = xs_y(trn_df)\nval_xs,val_y = xs_y(val_df)\n\n\ntrn_xs[:5]\n\n\n\n\n\n  \n    \n      \n      Sex\n      Embarked\n      Age\n      SibSp\n      Parch\n      LogFare\n      Pclass\n    \n  \n  \n    \n      298\n      1\n      2\n      24.00\n      0\n      0\n      3.449988\n      1\n    \n    \n      884\n      1\n      2\n      25.00\n      0\n      0\n      2.085672\n      3\n    \n    \n      247\n      0\n      2\n      24.00\n      0\n      2\n      2.740840\n      2\n    \n    \n      478\n      1\n      2\n      22.00\n      0\n      0\n      2.142510\n      3\n    \n    \n      305\n      1\n      2\n      0.92\n      1\n      2\n      5.027492\n      1\n    \n  \n\n\n\n\nHere’s the predictions for our extremely simple model, where female is coded as 0:\n\npreds = val_xs.Sex==0\n\nWe’ll use mean absolute error to measure how good this model is:\n\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(val_y, preds)\n\n0.21524663677130046\n\n\nAlternatively, we could try splitting on a continuous column. We have to use a somewhat different chart to see how this might work – here’s an example of how we could look at LogFare:\n\ndf_fare = trn_df[trn_df.LogFare>0]\nfig,axs = plt.subplots(1,2, figsize=(11,5))\nsns.boxenplot(data=df_fare, x=dep, y=\"LogFare\", ax=axs[0])\nsns.kdeplot(data=df_fare, x=\"LogFare\", ax=axs[1]);\n\n\n\n\nThe boxenplot above shows quantiles of LogFare for each group of Survived==0 and Survived==1. It shows that the average LogFare for passengers that didn’t survive is around 2.5, and for those that did it’s around 3.2. So it seems that people that paid more for their tickets were more likely to get put on a lifeboat.\nLet’s create a simple model based on this observation:\n\npreds = val_xs.LogFare>2.7\n\n…and test it out:\n\nmean_absolute_error(val_y, preds)\n\n0.336322869955157\n\n\nThis is quite a bit less accurate than our model that used Sex as the single binary split.\nIdeally, we’d like some way to try more columns and breakpoints more easily. We could create a function that returns how good our model is, in order to more quickly try out a few different splits. We’ll create a score function to do this. Instead of returning the mean absolute error, we’ll calculate a measure of impurity – that is, how much the binary split creates two groups where the rows in a group are each similar to each other, or dissimilar.\nWe can measure the similarity of rows inside a group by taking the standard deviation of the dependent variable. If it’s higher, then it means the rows are more different to each other. We’ll then multiply this by the number of rows, since a bigger group as more impact than a smaller group:\n\ndef _side_score(side, y):\n    tot = side.sum()\n    if tot<=1: return 0\n    return y[side].std()*tot\n\nNow we’ve got that written, we can calculate the score for a split by adding up the scores for the “left hand side” (lhs) and “right hand side” (rhs):\n\ndef score(col, y, split):\n    lhs = col<=split\n    return (_side_score(lhs,y) + _side_score(~lhs,y))/len(y) # ~ lhs means `not` left hand side\n\nFor instance, here’s the impurity score for the split on Sex:\n\nscore(trn_xs[\"Sex\"], trn_y, 0.5)\n\n0.40787530982063946\n\n\n…and for LogFare:\n\nscore(trn_xs[\"LogFare\"], trn_y, 2.7)\n\n0.47180873952099694\n\n\nAs we’d expect from our earlier tests, Sex appears to be a better split.\nTo make it easier to find the best binary split, we can create a simple interactive tool.\n\ndef iscore(nm, split):\n    col = trn_xs[nm]\n    return score(col, trn_y, split)\n\nfrom ipywidgets import interact\ninteract(nm=conts, split=15.5)(iscore);\n\n\n\n\nTry selecting different columns and split points using the dropdown and slider above. What splits can you find that increase the purity of the data?\nWe can do the same thing for the categorical variables:\n\ninteract(nm=cats, split=2)(iscore);\n\n\n\n\nThat works well enough, but it’s rather slow and fiddly. How do we automate it?\nWe could make a list of all possible splits and then calculate score and pick the best one.\nHere we can see all the possible splits for Age.\n\nnm = \"Age\"\ncol = trn_xs[nm]\nunq = col.unique()\nunq.sort()\nunq\n\narray([ 0.42,  0.67,  0.75,  0.83,  0.92,  1.  ,  2.  ,  3.  ,  4.  ,  5.  ,  6.  ,  7.  ,  8.  ,  9.  , 10.  , 11.  , 12.  ,\n       13.  , 14.  , 14.5 , 15.  , 16.  , 17.  , 18.  , 19.  , 20.  , 21.  , 22.  , 23.  , 24.  , 24.5 , 25.  , 26.  , 27.  ,\n       28.  , 28.5 , 29.  , 30.  , 31.  , 32.  , 32.5 , 33.  , 34.  , 34.5 , 35.  , 36.  , 36.5 , 37.  , 38.  , 39.  , 40.  ,\n       40.5 , 41.  , 42.  , 43.  , 44.  , 45.  , 45.5 , 46.  , 47.  , 48.  , 49.  , 50.  , 51.  , 52.  , 53.  , 54.  , 55.  ,\n       55.5 , 56.  , 57.  , 58.  , 59.  , 60.  , 61.  , 62.  , 64.  , 65.  , 70.  , 70.5 , 74.  , 80.  ])\n\n\nand now we just need to find which index of those values is where score() is the lowest:\n\nscores = np.array([score(col, trn_y, o) for o in unq if not np.isnan(o)])\nunq[scores.argmin()]\n\n6.0\n\n\nBased on this, it looks like, for instance, that for the Age column, 6 is the optimal cutoff according to our training set.\nWe can write a little function that implements this idea:\n\ndef min_col(df, nm):\n    col,y = df[nm],df[dep]\n    unq = col.dropna().unique()\n    scores = np.array([score(col, y, o) for o in unq if not np.isnan(o)])\n    idx = scores.argmin()\n    return unq[idx],scores[idx]\n\nmin_col(trn_df, \"Age\")\n\n(6.0, 0.478316717508991)\n\n\nLet’s try all the columns:\n\ncols = cats+conts\n{o:min_col(trn_df, o) for o in cols}\n\n{'Sex': (0, 0.40787530982063946),\n 'Embarked': (0, 0.47883342573147836),\n 'Age': (6.0, 0.478316717508991),\n 'SibSp': (4, 0.4783740258817434),\n 'Parch': (0, 0.4805296527841601),\n 'LogFare': (2.4390808375825834, 0.4620823937736597),\n 'Pclass': (2, 0.46048261885806596)}\n\n\nAccording to this, Sex<=0 is the best split we can use.\nWe’ve just re-invented the OneR classifier (or at least, a minor variant of it), which was found to be one of the most effective classifiers in real-world datasets, compared to the algorithms in use in 1993. Since it’s so simple and surprisingly effective, it makes for a great baseline – that is, a starting point that you can use to compare your more sophisticated models to.\nWe found earlier that out OneR rule had an error of around 0.215, so we’ll keep that in mind as we try out more sophisticated approaches."
  },
  {
    "objectID": "Projects/fastai22/part1/07-how-random-forests-really-work.html#creating-a-decision-tree",
    "href": "Projects/fastai22/part1/07-how-random-forests-really-work.html#creating-a-decision-tree",
    "title": "p1-07 How random forests really work",
    "section": "Creating a decision tree",
    "text": "Creating a decision tree\nHow can we improve our OneR classifier, which predicts survival based only on Sex?\nFirst, we’ll remove Sex from the list of possible splits (since we’ve already used it, and there’s only one possible split for that binary column), and create our two groups:\n\ncols.remove(\"Sex\")\nismale = trn_df.Sex==1\nmales,females = trn_df[ismale],trn_df[~ismale]\n\nNow let’s find the single best binary split for males…:\n\n{o:min_col(males, o) for o in cols}\n\n{'Embarked': (0, 0.3875581870410906),\n 'Age': (6.0, 0.3739828371010595),\n 'SibSp': (4, 0.3875864227586273),\n 'Parch': (0, 0.3874704821461959),\n 'LogFare': (2.803360380906535, 0.3804856231758151),\n 'Pclass': (1, 0.38155442004360934)}\n\n\n…and for females:\n\n{o:min_col(females, o) for o in cols}\n\n{'Embarked': (0, 0.4295252982857327),\n 'Age': (50.0, 0.4225927658431649),\n 'SibSp': (4, 0.42319212059713535),\n 'Parch': (3, 0.4193314500446158),\n 'LogFare': (4.256321678298823, 0.41350598332911376),\n 'Pclass': (2, 0.3335388911567601)}\n\n\nWe can see that the best next binary split for males is Age<=6, and for females is Pclass<=2.\nBy adding these rules, we have created a decision tree, where our model will first check whether Sex is female or male, and depending on the result will then check either the above Age or Pclass rules, as appropriate. We could then repeat the process, creating new additional rules for each of the four groups we’ve now created.\nRather than writing that code manually, we can use DecisionTreeClassifier, from sklearn, which does exactly that for us:\n\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\nm = DecisionTreeClassifier(max_leaf_nodes=4).fit(trn_xs, trn_y);\n\nOne handy feature or this class is that it provides a function for drawing a tree representing the rules:\n\nimport graphviz\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n                      special_characters=True, rotate=False, precision=precision, **kwargs)\n    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))\n\n\ndraw_tree(m, trn_xs)\n\n\n\n\nWe can see that it’s found exactly the same splits as we did!\nIn this picture, the more orange nodes have a lower survival rate, and blue have higher survival. Each node shows how many rows (“samples”) match that set of rules, and shows how many perish or survive (“values”). There’s also something called “gini”. That’s another measure of impurity, and it’s very similar to the score() we created earlier. It’s defined as follows:\n\ndef gini(cond):\n    act = df.loc[cond, dep]\n    return 1 - act.mean()**2 - (1-act).mean()**2\n\nWhat this calculates is the probability that, if you pick two rows from a group, you’ll get the same Survived result each time. If the group is all the same, the probability is 1.0, and 0.0 if they’re all different:\n\ngini(df.Sex=='female'), gini(df.Sex=='male')\n\n(0.3828350034484158, 0.3064437162277842)\n\n\nLet’s see how this model compares to our OneR version:\n\nmean_absolute_error(val_y, m.predict(val_xs))\n\n0.2242152466367713\n\n\nIt’s a tiny bit worse. Since this is such a small dataset (we’ve only got around 200 rows in our validation set) this small difference isn’t really meaningful. Perhaps we’ll see better results if we create a bigger tree:\n\nm = DecisionTreeClassifier(min_samples_leaf=50)\nm.fit(trn_xs, trn_y)\ndraw_tree(m, trn_xs, size=25)\n\n\n\n\n\nmean_absolute_error(val_y, m.predict(val_xs))\n\n0.18385650224215247\n\n\nIt looks like this is an improvement, although again it’s a bit hard to tell with small datasets like this. Let’s try submitting it to Kaggle:\n\ntst_df[cats] = tst_df[cats].apply(lambda x: x.cat.codes)\ntst_xs,_ = xs_y(tst_df)\n\ndef subm(preds, suff):\n    tst_df['Survived'] = preds\n    sub_df = tst_df[['PassengerId','Survived']]\n    sub_df.to_csv(f'sub-{suff}.csv', index=False)\n\n# subm(m.predict(tst_xs), 'tree')\n\nWhen I submitted this, I got a score of 0.765, which isn’t as good as our linear models or most of our neural nets, but it’s pretty close to those results.\nHopefully you can now see why we didn’t really need to create dummy variables, but instead just converted the labels into numbers using some (potentially arbitary) ordering of categories. For instance, here’s how the first few items of Embarked are labeled:\n\ndf.Embarked.head()\n\n0    S\n1    C\n2    S\n3    S\n4    S\nName: Embarked, dtype: category\nCategories (3, object): ['C', 'Q', 'S']\n\n\n…resulting in these integer codes:\n\ndf.Embarked.cat.codes.head()\n\n0    2\n1    0\n2    2\n3    2\n4    2\ndtype: int8\n\n\nSo let’s say we wanted to split into “C” in one group, vs “Q” or “S” in the other group. Then we just have to split on codes <=0 (since C is mapped to category 0). Note that if we wanted to split into “Q” in one group, we’d need to use two binary splits, first to separate “C” from “Q” and “S”, and then a second split to separate “Q” from “S”. For this reason, sometimes it can still be helpful to use dummy variables for categorical variables with few levels (like this one).\nIn practice, I often use dummy variables for <4 levels, and numeric codes for >=4 levels."
  },
  {
    "objectID": "Projects/fastai22/part1/07-how-random-forests-really-work.html#the-random-forest",
    "href": "Projects/fastai22/part1/07-how-random-forests-really-work.html#the-random-forest",
    "title": "p1-07 How random forests really work",
    "section": "The random forest",
    "text": "The random forest\nWe can’t make the decision tree much bigger than the example above, since some leaf nodes already have only 50 rows in them. That’s not a lot of data to make a prediction.\nSo how could we use bigger trees? One big insight came from Leo Breiman: what if we create lots of bigger trees, and take the average of their predictions? Taking the average prediction of a bunch of models in this way is known as bagging.\nThe idea is that we want each model’s predictions in the averaged ensemble to be uncorrelated with each other model. That way, if we average the predictions, the average will be equal to the true target value – that’s because the average of lots of uncorrelated random errors is zero. That’s quite an amazing insight!\nOne way we can create a bunch of uncorrelated models is to train each of them on a different random subset of the data. Here’s how we can create a tree on a random subset of the data:\n\ndef get_tree(prop=0.75):\n    n = len(trn_y)\n    idxs = random.choice(n, int(n*prop))\n    return DecisionTreeClassifier(min_samples_leaf=5).fit(trn_xs.iloc[idxs], trn_y.iloc[idxs])\n\nNow we can create as many trees as we want:\n\ntrees = [get_tree() for t in range(100)]\n\nOur prediction will be the average of these trees’ predictions:\n\nall_probs = [t.predict(val_xs) for t in trees]\navg_probs = np.stack(all_probs).mean(0)\n\nmean_absolute_error(val_y, avg_probs)\n\n0.2272645739910314\n\n\nThis is nearly identical to what sklearn’s RandomForestClassifier does. The main extra piece in a “real” random forest is that as well as choosing a random sample of data for each tree, it also picks a random subset of columns for each split. Here’s how we repeat the above process with a random forest:\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(100, min_samples_leaf=5)\nrf.fit(trn_xs, trn_y);\nmean_absolute_error(val_y, rf.predict(val_xs))\n\n0.18834080717488788\n\n\nWe can submit that to Kaggle too:\n\n# subm(rf.predict(tst_xs), 'rf')\n\nI found that gave nearly an identical result as our single tree (which, in turn, was slightly lower than our linear and neural net models in the previous notebook).\nOne particularly nice feature of random forests is they can tell us which independent variables were the most important in the model, using feature_importances_:\n\npd.DataFrame(dict(cols=trn_xs.columns, imp=m.feature_importances_)).plot('cols', 'imp', 'barh');\n\n\n\n\nWe can see that Sex is by far the most important predictor, with Pclass a distant second, and LogFare and Age behind that. In datasets with many columns, I generally recommend creating a feature importance plot as soon as possible, in order to find which columns are worth studying more closely. (Note also that we didn’t really need to take the log() of Fare, since random forests only care about order, and log() doesn’t change the order – we only did it to make our graphs earlier easier to read.)\nFor details about deriving and understanding feature importances, and the many other important diagnostic tools provided by random forests, take a look at chapter 8 of book."
  },
  {
    "objectID": "Projects/fastai22/part1/07-how-random-forests-really-work.html#conclusion",
    "href": "Projects/fastai22/part1/07-how-random-forests-really-work.html#conclusion",
    "title": "p1-07 How random forests really work",
    "section": "Conclusion",
    "text": "Conclusion\nSo what can we take away from all this?\nI think the first thing I’d note from this is that, clearly, more complex models aren’t always better. Our “OneR” model, consisting of a single binary split, was nearly as good as our more complex models. Perhaps in practice a simple model like this might be much easier to use, and could be worth considering. Our random forest wasn’t an improvement on the single decision tree at all.\nSo we should always be careful to benchmark simple models, as see if they’re good enough for our needs. In practice, you will often find that simple models will have trouble providing adequate accuracy for more complex tasks, such as recommendation systems, NLP, computer vision, or multivariate time series. But there’s no need to guess – it’s so easy to try a few different models, there’s no reason not to give the simpler ones a go too!\nAnother thing I think we can take away is that random forests aren’t actually that complicated at all. We were able to implement the key features of them in a notebook quite quickly. And they aren’t sensitive to issues like normalization, interactions, or non-linear transformations, which make them extremely easy to work with, and hard to mess up!"
  },
  {
    "objectID": "Projects/fastai22/part1/04-how-does-a-neural-net-really-work.html",
    "href": "Projects/fastai22/part1/04-how-does-a-neural-net-really-work.html",
    "title": "p1-04 How does a neural net really work",
    "section": "",
    "text": "Important: The interactive features of this notebook don’t work in Kaggle’s Reader mode. They only work in Edit mode. Therefore, before starting reading this, please click “Copy & Edit” in the top right of this window, then in the menu click Run and then Run all. Then you’ll be able to use all the interactive sliders in this notebook."
  },
  {
    "objectID": "Projects/fastai22/part1/04-how-does-a-neural-net-really-work.html#fitting-a-function-with-gradient-descent",
    "href": "Projects/fastai22/part1/04-how-does-a-neural-net-really-work.html#fitting-a-function-with-gradient-descent",
    "title": "p1-04 How does a neural net really work",
    "section": "Fitting a function with gradient descent",
    "text": "Fitting a function with gradient descent\nA neural network is just a mathematical function. In the most standard kind of neural network, the function:\n\nMultiplies each input by a number of values. These values are known as parameters\nAdds them up for each group of values\nReplaces the negative numbers with zeros\n\nThis represents one “layer”. Then these three steps are repeated, using the outputs of the previous layer as the inputs to the next layer. Initially, the parameters in this function are selected randomly. Therefore, a newly created neural network doesn’t do anything useful at all – it’s just random!\nTo get the function to “learn” to do something useful, we have to change the parameters to make them “better” in some way. We do this using gradient descent. Let’s see how this works…\n\nfrom ipywidgets import interact\nfrom fastai.basics import *\n\nplt.rc('figure', dpi=90)\n\ndef plot_function(f, title=None, min=-2.1, max=2.1, color='r', ylim=None):\n    x = torch.linspace(min,max, 100)[:,None]\n    if ylim: plt.ylim(ylim)\n    plt.plot(x, f(x), color)\n    if title is not None: plt.title(title)\n\nLet’s plot $3x^2+2x+1 $ quadratic function using plot_function\n\ndef f(x): return 3*x**2 + 2*x + 1\n\nplot_function(f, \"$3x^2 + 2x + 1$\")\n\n\n\n\nDefine quadratic as : $ Ax^2 + Bx + C $ as we learned in school\n\ndef quad(a, b, c, x): return a*x**2 + b*x + c\n\nTo fix values passed to a function in python, we use partial function.\n\ndef mk_quad(a,b,c): return partial(quad, a,b,c)\n\n\nf2 = mk_quad(3,2,1)\nplot_function(f2)\n\n\n\n\nNow, let’s add some noise to our measurements of quadratic.\n\ndef noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n\n\nnp.random.seed(42)\n\nx = torch.linspace(-2, 2, steps=20)[:,None]\ny = add_noise(f(x), 0.15, 1.5)\n\nLets see our noise\n\nx[:5],y[:5]\n\n(tensor([[-2.0000],\n         [-1.7895],\n         [-1.5789],\n         [-1.3684],\n         [-1.1579]]),\n tensor([[11.8690],\n         [ 6.5433],\n         [ 5.9396],\n         [ 2.6304],\n         [ 1.7947]], dtype=torch.float64))\n\n\nAs we can see, they’re tensors. A tensor is just like an array in NumPy\nLet’s take a look at the plot\n\nplt.scatter(x,y);\n\n\n\n\nNow using @interact we are trying to fit the function to our noised data\n\n@interact(a=1.1, b=1.1, c=1.1)\ndef plot_quad(a, b, c):\n    plt.scatter(x,y)\n    plot_function(mk_quad(a,b,c), ylim=(-3,13))\n\n\n\n\nAs a human we can kinda fit the function just by looking at it but for the computer to know if it’s making progress we need to create loss function, in the cell above we create a simple mae (mean absolute error) function to calculator the loss\n\ndef mae(preds, acts): return (torch.abs(preds-acts)).mean()\n\nNow fit the graph again with loss function displayed\n\n@interact(a=1.1, b=1.1, c=1.1)\ndef plot_quad(a, b, c):\n    f = mk_quad(a,b,c)\n    plt.scatter(x,y)\n    loss = mae(f(x), y)\n    plot_function(f, ylim=(-3,12), title=f\"MAE: {loss:.2f}\")"
  },
  {
    "objectID": "Projects/fastai22/part1/04-how-does-a-neural-net-really-work.html#automating-gradient-descent",
    "href": "Projects/fastai22/part1/04-how-does-a-neural-net-really-work.html#automating-gradient-descent",
    "title": "p1-04 How does a neural net really work",
    "section": "Automating gradient descent",
    "text": "Automating gradient descent\nThis simple Quadratic only has 3 parameters, but in a mordern neural network we will find often tens of milions of parameters to fit. We wont be albe to do that by hand so we need to automate the process.\nTo do this, we will find the gradient of mae() for each of our parameters, and then adjust our parameters.\nTo do this, we need a function that takes parameters a, b, c as a singe vector and returns mae()\n\ndef quad_mae(params):\n    f = mk_quad(*params)\n    return mae(f(x), y)\n\nLet’s try it:\n\nquad_mae([1.1, 1.1, 1.1])\n\ntensor(2.4219, dtype=torch.float64)\n\n\n\nabc = torch.tensor([1.1,1.1,1.1])\nabc\n\ntensor([1.1000, 1.1000, 1.1000])\n\n\nNow how do we update the parameters we will use a magic of gradients, lucky we don’t need to calculate them by hand ^^, we don’t even need to create a function since pytoch has a way to calculate them for us.\nTo tell PyTorch we want to calculate gradients for these parameters, we need to call requires_grad_()\n\nabc.requires_grad_()\n\ntensor([1.1000, 1.1000, 1.1000], requires_grad=True)\n\n\nNow we can calculate mae()\n\nloss = quad_mae(abc)\nloss\n\ntensor(2.4219, dtype=torch.float64, grad_fn=<MeanBackward0>)\n\n\nTo get PyTorch to calculate the gradients, we just need to call backwards()\n\nloss.backward()\n\nNow let’s see our gradients, there will be stored in an attribute called grad\n\nabc.grad\n\ntensor([-1.3529, -0.0316, -0.5000])\n\n\nAccording to these gradients, our parameters are a little low. Let’s increase them a bit. If we subtract the gradient, multiplied by a small number later known as LearningRate, that should improve Them\n\nwith torch.no_grad():\n    abc -= abc.grad*0.01 # This number is basicly Larning Rate\n    loss = quad_mae(abc)\n    \nprint(f'loss={loss:.2f}')\n\nloss=2.40\n\n\nJeremy’s NOTE\nBTW, you’ll see we had to wrap our calculation of the new parameters in with torch.no_grad(). That disables the calculation of gradients for any operations inside that context manager. We have to do that, because abc -= abc.grad*0.01 isn’t actually part of our quadratic model, so we don’t want derivatives to include that calculation.\nNice our loss already dropped now let’s do it more times\n\nfor i in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad(): abc -= abc.grad*0.01\n    print(f'step={i}; loss={loss:.2f}')\n\nstep=0; loss=2.40\nstep=1; loss=2.36\nstep=2; loss=2.30\nstep=3; loss=2.21\nstep=4; loss=2.11\nstep=5; loss=1.98\nstep=6; loss=1.85\nstep=7; loss=1.72\nstep=8; loss=1.58\nstep=9; loss=1.46\n\n\nOur loss dropped a lot, if we would keep running this loop for long enough, you’ll see the loss eventually start increasing for a while. That’s because once the parameters get close to the correct answer, our parameter updates will jump over the correct answer! To avoid this, we need to decrease our learning rate as we train. This is done using learning rate schedule and can be automated in most deep learning frameworks.\n\nabc\n\ntensor([1.9634, 1.1381, 1.4100], requires_grad=True)"
  },
  {
    "objectID": "Projects/fastai22/part1/04-how-does-a-neural-net-really-work.html#how-a-neural-network-approximates-any-given-function",
    "href": "Projects/fastai22/part1/04-how-does-a-neural-net-really-work.html#how-a-neural-network-approximates-any-given-function",
    "title": "p1-04 How does a neural net really work",
    "section": "How a neural network approximates any given function",
    "text": "How a neural network approximates any given function\nThe way a neural network approximates a any function turns out to be very simple. The key is using theRELU which is basically a linear function as u can see from code \\(y = mx * b\\), but we change all negative numbers to 0, with we do with troch.clip(y,0.)\n\ndef rectified_linear(m,b,x):\n    y = m*x+b\n    return torch.clip(y, 0.)\n\nHere’s what it looks like:\n\nplot_function(partial(rectified_linear, 1,1))\n\n\n\n\nInstead of torch.clip(y,0.) we can instead use F.relu(x) which does exactly the same thing. In PyTorch, F, refers to the torch.nn.functional module.\n\nimport torch.nn.functional as F\ndef rectified_linear2(m,b,x): return F.relu(m*x+b)\nplot_function(partial(rectified_linear2, 1,1))\n\n\n\n\nTo understand how RELU works/moves, try using interact, where m is the slope and b changes where the “hook” appears\n\n@interact(m=1.5, b=1.5)\ndef plot_relu(m, b):\n    plot_function(partial(rectified_linear, m,b), ylim=(-1,4))\n\n\n\n\nThe function doesn’t do much on its own, but look what happens when we add two of them\n\ndef double_relu(m1,b1,m2,b2,x):\n    return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)\n\n@interact(m1=-1.5, b1=-1.5, m2=1.5, b2=1.5)\ndef plot_double_relu(m1, b1, m2, b2):\n    plot_function(partial(double_relu, m1,b1,m2,b2), ylim=(-1,6))\n\n\n\n\nIf you play around with that for a while, you notice something quite profound: with enough of these rectified linear functions added together, you could approximate any function with a single input, to whatever accuracy you like! Any time the function doesn’t quite match, you can just add a few more additions to the mix to make it a bit closer."
  },
  {
    "objectID": "Projects/fastai22/part1/04-how-does-a-neural-net-really-work.html#how-to-recognise-an-owl",
    "href": "Projects/fastai22/part1/04-how-does-a-neural-net-really-work.html#how-to-recognise-an-owl",
    "title": "p1-04 How does a neural net really work",
    "section": "How to recognise an owl",
    "text": "How to recognise an owl\nOK great, we’ve created a nifty little example showing that we can drawing squiggly lines that go through some points. So what?\nWell… the truth is that actually drawing squiggly lines (or planes, or high-dimensional hyperplanes…) through some points is literally all that deep learning does! If your data points are, say, the RGB values of pixels in photos of owls, then you can create an owl-recogniser model by following the exact steps above.\nThis may, at first, sound about as useful as the classic “how to draw an owl” guide:\n\n\n\nimage.png\n\n\nGood read note from Jeremy\nStudents often ask me at this point, “OK Jeremy, but how do neural nets actually work”. But at a foundational level, there is no “step 2”. We’re done – the above steps will, given enough time and enough data, create (for example) an owl recognizer, if you feed in enough owls (and non-owls).\nThe devil, I guess, is in the “given enough time and enough data” part of the above sentence. There’s a lot of tweaks we can make to reduce both of these things. For instance, instead of running our calculations on a normal CPU, as we’ve done above, we could do thousands of them simultaneously by taking advantage of a GPU. We could greatly reduce the amount of computation and data needed by using a convolution instead of a matrix multiplication, which basically means skipping over a bunch of the multiplications and additions for bits that you’d guess won’t be important. We could make things much faster if, instead of starting with random parameters, we start with parameters of someone else’s model that does something similar to what we want (this is called transfer learning).\nAnd, of course, there’s lots of helpful software out there to do this stuff for you without too much fuss. Like, say, fastai.\nLearning these things is what we teach in our course, which, like everything we make, is totally free. So if you’re interested in learning more, do check it out!"
  },
  {
    "objectID": "Projects/fastai22/part1/02-saving-a-basic-fastai-model.html",
    "href": "Projects/fastai22/part1/02-saving-a-basic-fastai-model.html",
    "title": "p1-02 Saving a basic fastai model",
    "section": "",
    "text": "Make sure we have the latests version of fastai\n\n!pip install -Uqq fastai\n\nLet’s get all the necessary imports\n\nfrom fastai.vision.all import *\n\nGets the images path from the imported library and saves it as a path\n\npath = untar_data(URLs.PETS)/'images'\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 00:17<00:00]\n    \n    \n\n\nCreate a function that returns true if First letter of our file is uppercase if so that means there is a cat in the Image\n\ndef is_cat(x): return x[0].isupper()\n\nCreate DataLoaders for our model with ImageDataLoaders that gets all images files from path '.' current path and splits dataset into validation set to 80/20 with valid_pct=0.2, and we set seed = 42, then we resize our image to 192x192 size with no method so function defaults to crop other options are pad and squish\n\ndls = ImageDataLoaders.from_name_func('.',\n    get_image_files(path), valid_pct=0.2, seed=42,\n    label_func=is_cat,\n    item_tfms=Resize(192))\n\nCreates learner using vision learner given DataLoaders, Architecture resnet18 and metric = eror_rate Then it fine tunes the model for 3 epochs, fastai automatically finds the ideal lr in this example\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\n/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.192470\n      0.055552\n      0.020974\n      00:50\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.079966\n      0.087750\n      0.023004\n      00:48\n    \n    \n      1\n      0.050327\n      0.030392\n      0.008796\n      00:47\n    \n    \n      2\n      0.022960\n      0.027963\n      0.009472\n      00:48\n    \n  \n\n\n\nExports our model to pkl file using pickle liblary\n\nlearn.export('basic_model.pkl')\n\nNameError: name 'learn' is not defined\n\n\n\n!rm basic_model.pkl\n\nrm: cannot remove 'basic_model.pkl': No such file or directory"
  },
  {
    "objectID": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html",
    "href": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html",
    "title": "p1-05 Linear model and Neural Net from scratch",
    "section": "",
    "text": "In this notebook we’re going to build and train a deep learning model “from scratch” – by which I mean that we’re not going to use any pre-built architecture, or optimizers, or data loading frameworks, etc.\nWe’ll be assuming you already know the basics of how a neural network works. If you don’t, read this notebook first: How does a neural net really work?. We’ll be using Kaggle’s Titanic competition in this notebook, because it’s very small and simple, but also has displays many of the tricky real-life issues that we need to handle in most practical projects. (Note, however, that this competition is a small “learner” competition on Kaggle, so don’t expect to actually see much benefits from using a neural net just yet; that will come once we try our some real competitions!)\nIt’s great to be able to run the same notebook on your own machine or Colab, as well as Kaggle. To allow for this, we use this code to download the data as needed when not on Kaggle (see this notebook for details about this technique):\n\nimport os\nfrom pathlib import Path\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif iskaggle: path = Path('../input/titanic')\nelse:\n    path = Path('titanic')\n    if not path.exists():\n        import zipfile,kaggle\n        kaggle.api.competition_download_cli(str(path))\n        zipfile.ZipFile(f'{path}.zip').extractall(path)\n!rm titanic.zip\n\nDownloading titanic.zip to /root/Page/nbs/Projects/fastai22/part1\n\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34.1k/34.1k [00:00<00:00, 1.41MB/s]\n\n\n\n\n\n\n\n\nNote that the data for Kaggle comps always lives in the ../input folder. The easiest way to get the path is to click the “K” button in the top-right of the Kaggle notebook, click on the folder shown there, and click the copy button.\nWe’ll be using numpy and pytorch for array calculations in this notebook, and pandas for working with tabular data, so we’ll import them and set them to display using a bit more space than they default to.\n\nimport torch, numpy as np, pandas as pd\nnp.set_printoptions(linewidth=140)\ntorch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\npd.set_option('display.width', 140)"
  },
  {
    "objectID": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html#cleaning-the-data",
    "href": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html#cleaning-the-data",
    "title": "p1-05 Linear model and Neural Net from scratch",
    "section": "Cleaning the data",
    "text": "Cleaning the data\nReading in the data with pandas\n\ndf = pd.read_csv(path/'train.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      886\n      887\n      0\n      2\n      Montvila, Rev. Juozas\n      male\n      27.0\n      0\n      0\n      211536\n      13.0000\n      NaN\n      S\n    \n    \n      887\n      888\n      1\n      1\n      Graham, Miss. Margaret Edith\n      female\n      19.0\n      0\n      0\n      112053\n      30.0000\n      B42\n      S\n    \n    \n      888\n      889\n      0\n      3\n      Johnston, Miss. Catherine Helen \"Carrie\"\n      female\n      NaN\n      1\n      2\n      W./C. 6607\n      23.4500\n      NaN\n      S\n    \n    \n      889\n      890\n      1\n      1\n      Behr, Mr. Karl Howell\n      male\n      26.0\n      0\n      0\n      111369\n      30.0000\n      C148\n      C\n    \n    \n      890\n      891\n      0\n      3\n      Dooley, Mr. Patrick\n      male\n      32.0\n      0\n      0\n      370376\n      7.7500\n      NaN\n      Q\n    \n  \n\n891 rows × 12 columns\n\n\n\nChecking if our data has any missing values\n\ndf.isna().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\nAs we can see some of age and Cabin values are missing, we will handle that with filling them in with the mode of the value\n\nmodes = df.mode().iloc[0]\nmodes\n\nPassengerId                      1\nSurvived                       0.0\nPclass                         3.0\nName           Abbing, Mr. Anthony\nSex                           male\nAge                           24.0\nSibSp                          0.0\nParch                          0.0\nTicket                        1601\nFare                          8.05\nCabin                      B96 B98\nEmbarked                         S\nName: 0, dtype: object\n\n\nHe are the modes for every value\nand we fill in the missing values\n\ndf.fillna(modes, inplace=True)\n\n\ndf.isna().sum()\n\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nCabin          0\nEmbarked       0\ndtype: int64\n\n\nHere we can see how our numerical values are distributed with describe() method, include parameters ensures we only see numeric values\n\ndf.describe(include=(np.number))\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Age\n      SibSp\n      Parch\n      Fare\n    \n  \n  \n    \n      count\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n    \n    \n      mean\n      446.000000\n      0.383838\n      2.308642\n      28.566970\n      0.523008\n      0.381594\n      32.204208\n    \n    \n      std\n      257.353842\n      0.486592\n      0.836071\n      13.199572\n      1.102743\n      0.806057\n      49.693429\n    \n    \n      min\n      1.000000\n      0.000000\n      1.000000\n      0.420000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      223.500000\n      0.000000\n      2.000000\n      22.000000\n      0.000000\n      0.000000\n      7.910400\n    \n    \n      50%\n      446.000000\n      0.000000\n      3.000000\n      24.000000\n      0.000000\n      0.000000\n      14.454200\n    \n    \n      75%\n      668.500000\n      1.000000\n      3.000000\n      35.000000\n      1.000000\n      0.000000\n      31.000000\n    \n    \n      max\n      891.000000\n      1.000000\n      3.000000\n      80.000000\n      8.000000\n      6.000000\n      512.329200\n    \n  \n\n\n\n\nFrom the describe we can see that Fare has max value of 512 while the min value is 0 and 75% of the Fare’s are below 31 lets see Fare distribution on the histogram.\n\ndf['Fare'].hist();\n\n\n\n\nAs w can see most of our values are low numbers with some values a lot higher, it is actually common distribution in monetary values. One way to deal with this problem is to take a log of values.\nWe will actually take a log(a+1) since log of number lower than 1 is negative number, and we want our numbers to all be non negative\n\ndf['LogFare'] = np.log(df['Fare']+1)\n\n\ndf['LogFare'].hist();\n\n\n\n\nOur distribution look a lot better.\nIt looks like from the describe() output, the Pclass has only 3 values, which we can confirm by looking at Pclass.unique()\n\npclasses = sorted(df.Pclass.unique())\npclasses\n\n[1, 2, 3]\n\n\nHere’s how we get a quick summary of all the non-numeric columns in the dataset:\n\ndf.describe(include=[object])\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Ticket\n      Cabin\n      Embarked\n    \n  \n  \n    \n      count\n      891\n      891\n      891\n      891\n      891\n    \n    \n      unique\n      891\n      2\n      681\n      147\n      3\n    \n    \n      top\n      Braund, Mr. Owen Harris\n      male\n      347082\n      B96 B98\n      S\n    \n    \n      freq\n      1\n      577\n      7\n      691\n      646\n    \n  \n\n\n\n\nSince we can’t multiply strings like Sex male by a parameter, we need to replace them with numbers.\nWith Sex column it is pretty simple, just replace male with 1 and female with 0.\nFor the other values we will create additional column saying for example is this Embarked: S if yes just set value to 1 else 0\nPandas can create these automatically with get_dummies method, which also removes original columns. We’ll create dummy variables for Pclass, even although it’s numeric, since the numbers 1, 2, and 3 correspond to first, second, and third class cabins - not to counts or measures that make sense to multiply by. We’ll also create dummies for Sex and Embarked since we’ll want to use those as predictors in our model. On the other hand, Cabin, Name, and Ticket have too many unique values for it to make sense creating dummy variables for them.\n\ndf = pd.get_dummies(df, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\ndf.columns\n\nIndex(['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'LogFare', 'Sex_female', 'Sex_male',\n       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n      dtype='object')\n\n\nHere is what the added columns look like\n\nadded_cols = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\ndf[added_cols].head()\n\n\n\n\n\n  \n    \n      \n      Sex_male\n      Sex_female\n      Pclass_1\n      Pclass_2\n      Pclass_3\n      Embarked_C\n      Embarked_Q\n      Embarked_S\n    \n  \n  \n    \n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      0\n    \n    \n      2\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      3\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n    \n    \n      4\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n    \n  \n\n\n\n\nHere we create our independent(predictor)and the dependent(target) the value we want our model to predict. We want both of them to be tensors.\n\nfrom torch import tensor\n\nt_dep = tensor(df.Survived)\n\n\nindep_cols = ['Age', 'SibSp', 'Parch', 'LogFare'] + added_cols\n\nt_indep = tensor(df[indep_cols].values, dtype=torch.float)\nt_indep\n\ntensor([[22.0000,  1.0000,  0.0000,  2.1102,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [38.0000,  1.0000,  0.0000,  4.2806,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n        [26.0000,  0.0000,  0.0000,  2.1889,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [35.0000,  1.0000,  0.0000,  3.9908,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n        [35.0000,  0.0000,  0.0000,  2.2028,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [24.0000,  0.0000,  0.0000,  2.2469,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000],\n        [54.0000,  0.0000,  0.0000,  3.9677,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n        ...,\n        [25.0000,  0.0000,  0.0000,  2.0857,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [39.0000,  0.0000,  5.0000,  3.4054,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000],\n        [27.0000,  0.0000,  0.0000,  2.6391,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n        [19.0000,  0.0000,  0.0000,  3.4340,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n        [24.0000,  1.0000,  2.0000,  3.1966,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [26.0000,  0.0000,  0.0000,  3.4340,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n        [32.0000,  0.0000,  0.0000,  2.1691,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000]])\n\n\nHere is the number of rows and columns we have.\n\nt_indep.shape\n\ntorch.Size([891, 12])"
  },
  {
    "objectID": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html#setting-up-a-linear-model",
    "href": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html#setting-up-a-linear-model",
    "title": "p1-05 Linear model and Neural Net from scratch",
    "section": "Setting up a linear model",
    "text": "Setting up a linear model\nNow that we’ve got a matrix of independent and dependent values, we can work on calculating our predictions and our loss.\nOur first model will be a simple linear model. We’ll need a coefficient(Parameter) for each column in t_indep. We’ll assign it a random number between -0.5 and 0.5.\n\ntorch.manual_seed(442)\n\nn_coeff = t_indep.shape[1]\ncoeffs = torch.rand(n_coeff)-0.5\ncoeffs\n\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392,  0.2103,  0.3625])\n\n\nWe created our coeffs [12,1] tensor.\nHere is what the multiplication looks like.\n\nt_indep*coeffs\n\ntensor([[-10.1838,   0.1386,   0.0000,  -0.4772,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-17.5902,   0.1386,   0.0000,  -0.9681,  -0.0000,  -0.3147,   0.4876,   0.0000,   0.0000,  -0.4392,   0.0000,   0.0000],\n        [-12.0354,   0.0000,   0.0000,  -0.4950,  -0.0000,  -0.3147,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-16.2015,   0.1386,   0.0000,  -0.9025,  -0.0000,  -0.3147,   0.4876,   0.0000,   0.0000,  -0.0000,   0.0000,   0.3625],\n        [-16.2015,   0.0000,   0.0000,  -0.4982,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-11.1096,   0.0000,   0.0000,  -0.5081,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.2103,   0.0000],\n        [-24.9966,   0.0000,   0.0000,  -0.8973,  -0.2632,  -0.0000,   0.4876,   0.0000,   0.0000,  -0.0000,   0.0000,   0.3625],\n        ...,\n        [-11.5725,   0.0000,   0.0000,  -0.4717,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-18.0531,   0.0000,   1.2045,  -0.7701,  -0.0000,  -0.3147,   0.0000,   0.0000,   0.2799,  -0.0000,   0.2103,   0.0000],\n        [-12.4983,   0.0000,   0.0000,  -0.5968,  -0.2632,  -0.0000,   0.0000,   0.3136,   0.0000,  -0.0000,   0.0000,   0.3625],\n        [ -8.7951,   0.0000,   0.0000,  -0.7766,  -0.0000,  -0.3147,   0.4876,   0.0000,   0.0000,  -0.0000,   0.0000,   0.3625],\n        [-11.1096,   0.1386,   0.4818,  -0.7229,  -0.0000,  -0.3147,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-12.0354,   0.0000,   0.0000,  -0.7766,  -0.2632,  -0.0000,   0.4876,   0.0000,   0.0000,  -0.4392,   0.0000,   0.0000],\n        [-14.8128,   0.0000,   0.0000,  -0.4905,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.2103,   0.0000]])\n\n\n\nt_indep.max(dim =0)\n\ntorch.return_types.max(\nvalues=tensor([80.0000,  8.0000,  6.0000,  6.2409,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000]),\nindices=tensor([630, 159, 678, 258,   0,   1,   1,   9,   0,   1,   5,   0]))\n\n\nWe see we’ve got a problem here. The sum of each row will be dominated by the Age column, since it’s bigger on the average.\nLet’s make all the columns contain numbers from 0 to 1, by dividing each column by its max():\n\nvals,indices = t_indep.max(dim=0)\nt_indep = t_indep / vals\n\n\nt_indep*coeffs\n\ntensor([[-0.1273,  0.0173,  0.0000, -0.0765, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.2199,  0.0173,  0.0000, -0.1551, -0.0000, -0.3147,  0.4876,  0.0000,  0.0000, -0.4392,  0.0000,  0.0000],\n        [-0.1504,  0.0000,  0.0000, -0.0793, -0.0000, -0.3147,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.2025,  0.0173,  0.0000, -0.1446, -0.0000, -0.3147,  0.4876,  0.0000,  0.0000, -0.0000,  0.0000,  0.3625],\n        [-0.2025,  0.0000,  0.0000, -0.0798, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.1389,  0.0000,  0.0000, -0.0814, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.2103,  0.0000],\n        [-0.3125,  0.0000,  0.0000, -0.1438, -0.2632, -0.0000,  0.4876,  0.0000,  0.0000, -0.0000,  0.0000,  0.3625],\n        ...,\n        [-0.1447,  0.0000,  0.0000, -0.0756, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.2257,  0.0000,  0.2008, -0.1234, -0.0000, -0.3147,  0.0000,  0.0000,  0.2799, -0.0000,  0.2103,  0.0000],\n        [-0.1562,  0.0000,  0.0000, -0.0956, -0.2632, -0.0000,  0.0000,  0.3136,  0.0000, -0.0000,  0.0000,  0.3625],\n        [-0.1099,  0.0000,  0.0000, -0.1244, -0.0000, -0.3147,  0.4876,  0.0000,  0.0000, -0.0000,  0.0000,  0.3625],\n        [-0.1389,  0.0173,  0.0803, -0.1158, -0.0000, -0.3147,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.1504,  0.0000,  0.0000, -0.1244, -0.2632, -0.0000,  0.4876,  0.0000,  0.0000, -0.4392,  0.0000,  0.0000],\n        [-0.1852,  0.0000,  0.0000, -0.0786, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.2103,  0.0000]])\n\n\nNow it look a lot better.\nNow we can make prediction from our linear model, by adding up the rows of the product\nNOTE for axis * if 0 - > return sum of columns * if 1 - > return sum of the rows\n\npreds = (t_indep*coeffs).sum(axis=1)\n\nLet’s take a look at the first few:\n\npreds[:10]\n\ntensor([ 0.1927, -0.6239,  0.0979,  0.2056,  0.0968,  0.0066,  0.1306,  0.3476,  0.1613, -0.6285])\n\n\nOf course, those predictions aren’t good since our coefficient were random. To do the gradient descent, we need a loss function.\n\nloss = torch.abs(preds-t_dep).mean()\nloss\n\ntensor(0.5382)\n\n\nNow that we’ve tested out a way of calculating predictions, and loss, let’s pop them into functions to make life easier:\n\ndef calc_preds(coeffs, indeps): return (indeps*coeffs).sum(axis=1)\ndef calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()"
  },
  {
    "objectID": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html#training-the-linear-model",
    "href": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html#training-the-linear-model",
    "title": "p1-05 Linear model and Neural Net from scratch",
    "section": "Training the linear model",
    "text": "Training the linear model\nLet’s import RandomSplitter from fastai and split our data into training and validation set\n\nfrom fastai.data.transforms import RandomSplitter\ntrn_split,val_split=RandomSplitter(seed=42)(df)\n\nNow we can apply those indicies to our independent and dependent variables:\n\ntrn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\ntrn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\nlen(trn_indep),len(val_indep)\n\n(713, 178)\n\n\nLest’s create a function to make things easier\n\ndef update_coeffs(coeffs, lr):\n    coeffs.sub_(coeffs.grad * lr)\n    coeffs.grad.zero_()\n\n\ndef one_epoch(coeffs, lr):\n    loss = calc_loss(coeffs, trn_indep, trn_dep)\n    loss.backward()\n    with torch.no_grad(): update_coeffs(coeffs, lr)\n    print(f\"{loss:.3f}\", end=\"; \")\n\nReminder this function will initialize coeffs\n\ndef init_coeffs(): return (torch.rand(n_coeff)-0.5).requires_grad_()\n\nWe can now use these functions to train our model:\n\ndef train_model(epochs=30, lr=0.01):\n    torch.manual_seed(442)\n    coeffs = init_coeffs()\n    for i in range(epochs): one_epoch(coeffs, lr=lr)\n    return coeffs\n\nLet’s train our model for 18 epochs with lr = 0.2\n\ncoeffs = train_model(18, lr=0.2)\n\n0.536; 0.502; 0.477; 0.454; 0.431; 0.409; 0.388; 0.367; 0.349; 0.336; 0.330; 0.326; 0.329; 0.304; 0.314; 0.296; 0.300; 0.289; \n\n\nIt does!\nLet’s take a look at the coefficients for each column:\n\ndef show_coeffs(): return dict(zip(indep_cols, coeffs.requires_grad_(False)))\nshow_coeffs()\n\n{'Age': tensor(-0.2694),\n 'SibSp': tensor(0.0901),\n 'Parch': tensor(0.2359),\n 'LogFare': tensor(0.0280),\n 'Sex_male': tensor(-0.3990),\n 'Sex_female': tensor(0.2345),\n 'Pclass_1': tensor(0.7232),\n 'Pclass_2': tensor(0.4112),\n 'Pclass_3': tensor(0.3601),\n 'Embarked_C': tensor(0.0955),\n 'Embarked_Q': tensor(0.2395),\n 'Embarked_S': tensor(0.2122)}"
  },
  {
    "objectID": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html#measuring-accuracy",
    "href": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html#measuring-accuracy",
    "title": "p1-05 Linear model and Neural Net from scratch",
    "section": "Measuring accuracy",
    "text": "Measuring accuracy\nThe Kaggle competition is not, however, scored by absolute error (which is our loss function). It’s scored by accuracy – the proportion of rows where we correctly predict survival. Let’s see how accurate we were on the validation set. First, calculate the predictions:\n\npreds = calc_preds(coeffs, val_indep)\n\nWe’ll assume that any passenger with a score of over 0.5 is predicted to survive. So that means we’re correct for each row where preds>0.5 is the same as the dependent variable:\n\nresults = val_dep.bool()==(preds>0.5)\nresults[:16]\n\ntensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False, False, False,  True,  True, False])\n\n\nLet’s see what our average accuracy is:\n\nresults.float().mean()\n\ntensor(0.7865)\n\n\nThat’s not a bad start at all! We’ll create a function so we can calcuate the accuracy easy for other models we train:\n\ndef acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs, val_indep)>0.5)).float().mean()\nacc(coeffs)\n\ntensor(0.7865)"
  },
  {
    "objectID": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html#using-sigmoid",
    "href": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html#using-sigmoid",
    "title": "p1-05 Linear model and Neural Net from scratch",
    "section": "Using sigmoid",
    "text": "Using sigmoid\nLooking at our predictions,we can see that some of the values are >1 and some of them are <0\n\npreds[:28]\n\ntensor([ 0.8160,  0.1295, -0.0148,  0.1831,  0.1520,  0.1350,  0.7279,  0.7754,  0.3222,  0.6740,  0.0753,  0.0389,  0.2216,  0.7631,\n         0.0678,  0.3997,  0.3324,  0.8278,  0.1078,  0.7126,  0.1023,  0.3627,  0.9937,  0.8050,  0.1153,  0.1455,  0.8652,  0.3425])\n\n\nTo fix this we can put our prediction into sigmoid function, which has minimum of 0 and maximum of 1.\nHere is what is looks like\n\nimport sympy\nsympy.plot(\"1/(1+exp(-x))\", xlim=(-5,5));\n\n\n\n\nPytorch already defines that function for us, so we just need to modify the function\n\ndef calc_preds(coeffs, indeps): return torch.sigmoid((indeps*coeffs).sum(axis=1))\n\n\ncoeffs = train_model(lr=100)\n\n0.510; 0.327; 0.294; 0.207; 0.201; 0.199; 0.198; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; \n\n\n\nacc(coeffs)\n\ntensor(0.8258)\n\n\nThe accuracy went up by a lot! Let’s the coeffs\n\nshow_coeffs()\n\n{'Age': tensor(-1.5061),\n 'SibSp': tensor(-1.1575),\n 'Parch': tensor(-0.4267),\n 'LogFare': tensor(0.2543),\n 'Sex_male': tensor(-10.3320),\n 'Sex_female': tensor(8.4185),\n 'Pclass_1': tensor(3.8389),\n 'Pclass_2': tensor(2.1398),\n 'Pclass_3': tensor(-6.2331),\n 'Embarked_C': tensor(1.4771),\n 'Embarked_Q': tensor(2.1168),\n 'Embarked_S': tensor(-4.7958)}"
  },
  {
    "objectID": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html#submitting-to-kaggle",
    "href": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html#submitting-to-kaggle",
    "title": "p1-05 Linear model and Neural Net from scratch",
    "section": "Submitting to Kaggle",
    "text": "Submitting to Kaggle\nNow that we’ve got a trained model, we can prepare a submission to Kaggle. To do that, first we need to read the test set:\n\ntst_df = pd.read_csv(path/'test.csv')\n\n\ntst_df['Fare'] = tst_df.Fare.fillna(0)\n\nNow we can just copy the same steps we did to our training set and do the same exact things on our test set to preprocess the data:\n\ntst_df.fillna(modes, inplace=True)\ntst_df['LogFare'] = np.log(tst_df['Fare']+1)\ntst_df = pd.get_dummies(tst_df, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\n\ntst_indep = tensor(tst_df[indep_cols].values, dtype=torch.float)\ntst_indep = tst_indep / vals\n\n\ntst_df['Survived'] = (calc_preds(tst_indep, coeffs)>0.5).int()\n\n\nsub_df = tst_df[['PassengerId','Survived']]\nsub_df.to_csv('sub.csv', index=False)\n\nWe can check the first few rows of the file to make sure it looks reasonable:\n\n!head sub.csv\n\nPassengerId,Survived\n892,0\n893,0\n894,0\n895,0\n896,0\n897,0\n898,1\n899,0\n900,1\n\n\nWhen you click “save version” in Kaggle, and wait for the notebook to run, you’ll see that sub.csv appears in the “Data” tab. Clicking on that file will show a Submit button, which allows you to submit to the competition."
  },
  {
    "objectID": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html#using-matrix-product",
    "href": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html#using-matrix-product",
    "title": "p1-05 Linear model and Neural Net from scratch",
    "section": "Using matrix product",
    "text": "Using matrix product\nWe can make things quite a bit neater…\nTake a look at the inner-most calculation we’re doing to get the predictions:\n\n(val_indep*coeffs).sum(axis=1)\n\ntensor([ 12.3288, -14.8119, -15.4540, -13.1513, -13.3512, -13.6469,   3.6248,   5.3429, -22.0878,   3.1233, -21.8742, -15.6421, -21.5504,\n          3.9393, -21.9190, -12.0010, -12.3775,   5.3550, -13.5880,  -3.1015, -21.7237, -12.2081,  12.9767,   4.7427, -21.6525, -14.9135,\n         -2.7433, -12.3210, -21.5886,   3.9387,   5.3890,  -3.6196, -21.6296, -21.8454,  12.2159,  -3.2275, -12.0289,  13.4560, -21.7230,\n         -3.1366, -13.2462, -21.7230, -13.6831,  13.3092, -21.6477,  -3.5868, -21.6854, -21.8316, -14.8158,  -2.9386,  -5.3103, -22.2384,\n        -22.1097, -21.7466, -13.3780, -13.4909, -14.8119, -22.0690, -21.6666, -21.7818,  -5.4439, -21.7407, -12.6551, -21.6671,   4.9238,\n        -11.5777, -13.3323, -21.9638, -15.3030,   5.0243, -21.7614,   3.1820, -13.4721, -21.7170, -11.6066, -21.5737, -21.7230, -11.9652,\n        -13.2382, -13.7599, -13.2170,  13.1347, -21.7049, -21.7268,   4.9207,  -7.3198,  -5.3081,   7.1065,  11.4948, -13.3135, -21.8723,\n        -21.7230,  13.3603, -15.5670,   3.4105,  -7.2857, -13.7197,   3.6909,   3.9763, -14.7227, -21.8268,   3.9387, -21.8743, -21.8367,\n        -11.8518, -13.6712, -21.8299,   4.9440,  -5.4471, -21.9666,   5.1333,  -3.2187, -11.6008,  13.7920, -21.7230,  12.6369,  -3.7268,\n        -14.8119, -22.0637,  12.9468, -22.1610,  -6.1827, -14.8119,  -3.2838, -15.4540, -11.6950,  -2.9926,  -3.0110, -21.5664, -13.8268,\n          7.3426, -21.8418,   5.0744,   5.2582,  13.3415, -21.6289, -13.9898, -21.8112,  -7.3316,   5.2296, -13.4453,  12.7891, -22.1235,\n        -14.9625,  -3.4339,   6.3089, -21.9839,   3.1968,   7.2400,   2.8558,  -3.1187,   3.7965,   5.4667, -15.1101, -15.0597, -22.9391,\n        -21.7230,  -3.0346, -13.5206, -21.7011,  13.4425,  -7.2690, -21.8335, -12.0582,  13.0489,   6.7993,   5.2160,   5.0794, -12.6957,\n        -12.1838,  -3.0873, -21.6070,   7.0744, -21.7170, -22.1001,   6.8159, -11.6002, -21.6310])\n\n\nMultiplying elements together and then adding across rows is identical to doing a matrix-vector product! Python uses the @ operator to indicate matrix products, and is supported by PyTorch tensors. Therefore, we can replicate the above calculate more simply like so:\n\nval_indep@coeffs\n\ntensor([ 12.3288, -14.8119, -15.4540, -13.1513, -13.3511, -13.6468,   3.6248,   5.3429, -22.0878,   3.1233, -21.8742, -15.6421, -21.5504,\n          3.9393, -21.9190, -12.0010, -12.3775,   5.3550, -13.5880,  -3.1015, -21.7237, -12.2081,  12.9767,   4.7427, -21.6525, -14.9135,\n         -2.7433, -12.3210, -21.5886,   3.9387,   5.3890,  -3.6196, -21.6296, -21.8454,  12.2159,  -3.2275, -12.0289,  13.4560, -21.7230,\n         -3.1366, -13.2462, -21.7230, -13.6831,  13.3092, -21.6477,  -3.5868, -21.6854, -21.8316, -14.8158,  -2.9386,  -5.3103, -22.2384,\n        -22.1097, -21.7466, -13.3780, -13.4909, -14.8119, -22.0690, -21.6666, -21.7818,  -5.4439, -21.7407, -12.6551, -21.6671,   4.9238,\n        -11.5777, -13.3323, -21.9638, -15.3030,   5.0243, -21.7614,   3.1820, -13.4721, -21.7170, -11.6066, -21.5737, -21.7230, -11.9652,\n        -13.2382, -13.7599, -13.2170,  13.1347, -21.7049, -21.7268,   4.9207,  -7.3198,  -5.3081,   7.1065,  11.4948, -13.3135, -21.8723,\n        -21.7230,  13.3603, -15.5670,   3.4105,  -7.2857, -13.7197,   3.6909,   3.9763, -14.7227, -21.8268,   3.9387, -21.8743, -21.8367,\n        -11.8518, -13.6712, -21.8299,   4.9440,  -5.4471, -21.9666,   5.1333,  -3.2187, -11.6008,  13.7920, -21.7230,  12.6369,  -3.7268,\n        -14.8119, -22.0637,  12.9468, -22.1610,  -6.1827, -14.8119,  -3.2838, -15.4540, -11.6950,  -2.9926,  -3.0110, -21.5664, -13.8268,\n          7.3426, -21.8418,   5.0744,   5.2582,  13.3415, -21.6289, -13.9898, -21.8112,  -7.3316,   5.2296, -13.4453,  12.7891, -22.1235,\n        -14.9625,  -3.4339,   6.3089, -21.9839,   3.1968,   7.2400,   2.8558,  -3.1187,   3.7965,   5.4667, -15.1101, -15.0597, -22.9391,\n        -21.7230,  -3.0346, -13.5206, -21.7011,  13.4425,  -7.2690, -21.8335, -12.0582,  13.0489,   6.7993,   5.2160,   5.0794, -12.6957,\n        -12.1838,  -3.0873, -21.6070,   7.0744, -21.7170, -22.1001,   6.8159, -11.6002, -21.6310])\n\n\nAs u can see the output is identical, but using @ operator makes the matrix multiply much faster.\n\ndef calc_preds(coeffs, indeps): return torch.sigmoid(indeps@coeffs)\n\n\ndef init_coeffs(): return (torch.rand(n_coeff, 1)*0.1).requires_grad_()\n\n\ntrn_dep = trn_dep[:,None]\nval_dep = val_dep[:,None]\n\nWe can now train our model as before and confirm we get identical outputs…:\n\ncoeffs = train_model(lr=100)\n\n0.512; 0.323; 0.290; 0.205; 0.200; 0.198; 0.197; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; \n\n\n…and identical accuracy:\n\nacc(coeffs)\n\ntensor(0.8258)"
  },
  {
    "objectID": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html#a-neural-network",
    "href": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html#a-neural-network",
    "title": "p1-05 Linear model and Neural Net from scratch",
    "section": "A neural network",
    "text": "A neural network\nWe’ve now got what we need to implement our neural network.\nFirst, we’ll need to create coefficients for each of our layers. Our first set of coefficients will take our n_coeff inputs, and create n_hidden outputs. We can choose whatever n_hidden we like – a higher number gives our network more flexibility, but makes it slower and harder to train. So we need a matrix of size n_coeff by n_hidden. We’ll divide these coefficients by n_hidden so that when we sum them up in the next layer we’ll end up with similar magnitude numbers to what we started with.\nThen our second layer will need to take the n_hidden inputs and create a single output, so that means we need a n_hidden by 1 matrix there. The second layer will also need a constant term added.\n\nn_hidden = 20\n(torch.rand(n_coeff, n_hidden)) # n_coeffs - number of rows, n_hidden - number of coeffs in a row\ntorch.rand(n_hidden,1) #\n\ntensor([0.7839, 0.4441, 0.7304, 0.4664, 0.4205, 0.9709, 0.0276, 0.5295, 0.7127, 0.7399, 0.9193, 0.9949, 0.9180, 0.6263, 0.3220, 0.2740,\n        0.9050, 0.5448, 0.1600, 0.1784])\n\n\n\ndef init_coeffs(n_hidden=20):\n    layer1 = (torch.rand(n_coeff, n_hidden)-0.5)/n_hidden\n    layer2 = torch.rand(n_hidden, 1)-0.3\n    const = torch.rand(1)[0]\n    return layer1.requires_grad_(),layer2.requires_grad_(),const.requires_grad_()\n\nNow we have our coefficients, we can create our neural net. The key steps are the two matrix products, indeps@l1 and res@l2 (where res is the output of the first layer). The first layer output is passed to F.relu (that’s our non-linearity), and the second is passed to torch.sigmoid as before.\n\nimport torch.nn.functional as F\n\ndef calc_preds(coeffs, indeps):\n    l1,l2,const = coeffs\n    res = F.relu(indeps@l1)\n    res = res@l2 + const\n    return torch.sigmoid(res)\n\nFinally, now that we have more than one set of coefficients, we need to add a loop to update each one:\n\ndef update_coeffs(coeffs, lr):\n    for layer in coeffs:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\nThat’s it – we’re now ready to train our model!\n\ncoeffs = train_model(lr=1.4)\nacc(coeffs)\n\n0.543; 0.532; 0.520; 0.505; 0.487; 0.466; 0.439; 0.407; 0.373; 0.343; 0.319; 0.301; 0.286; 0.274; 0.264; 0.256; 0.250; 0.245; 0.240; 0.237; 0.234; 0.231; 0.229; 0.227; 0.226; 0.224; 0.223; 0.222; 0.221; 0.220; \n\n\ntensor(0.7921)\n\n\n\ncoeffs = train_model(lr=20)\nacc(coeffs)\n\n0.543; 0.400; 0.260; 0.390; 0.221; 0.211; 0.197; 0.195; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; \n\n\ntensor(0.8258)\n\n\nIt’s looking good\nIn this case our neural net isn’t showing better results than the linear model. That’s not surprising; this dataset is very small and very simple, and isn’t the kind of thing we’d expect to see neural networks excel at. Furthermore, our validation set is too small to reliably see much accuracy difference. But the key thing is that we now know exactly what a real neural net looks like!"
  },
  {
    "objectID": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html#deep-learning",
    "href": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html#deep-learning",
    "title": "p1-05 Linear model and Neural Net from scratch",
    "section": "Deep learning",
    "text": "Deep learning\nThe neural net in the previous section only uses one hidden layer, so it doesn’t count as “deep” learning. But we can use the exact same technique to make our neural net deep, by adding more matrix multiplications.\nFirst, we’ll need to create additional coefficients for each layer:\nUnderstanding init_coeffs\n\nsizes = [12] + [10,10] + [1]\nsizes\n\n[12, 10, 10, 1]\n\n\n\nn = len(sizes)\nly = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\nly\n\n[tensor([[-0.0885, -0.0638,  0.0772, -0.0345,  0.0909,  0.1749,  0.0677,  0.0966,  0.1951,  0.2063],\n         [-0.0830,  0.1576,  0.0236, -0.0339, -0.0435,  0.1856,  0.0527, -0.0925,  0.2777,  0.2286],\n         [ 0.2143,  0.2019, -0.1154, -0.1139,  0.1705,  0.1669,  0.0929,  0.0492,  0.0093,  0.0219],\n         [ 0.2064,  0.0697,  0.1346,  0.0099,  0.0619,  0.1576,  0.1706,  0.2313,  0.1813,  0.1084],\n         [-0.0234,  0.1411,  0.1117,  0.2572,  0.1412,  0.0425, -0.0518,  0.2002,  0.2679,  0.1291],\n         [ 0.0778, -0.0274, -0.0914,  0.0320,  0.0827,  0.2084, -0.0432, -0.0596, -0.1159,  0.1561],\n         [-0.0663,  0.0115, -0.0512, -0.0736,  0.1799, -0.1023,  0.2382,  0.1505, -0.1048, -0.1080],\n         [ 0.0528,  0.1592,  0.1679,  0.1505,  0.2484, -0.0191, -0.0484,  0.1934,  0.1189,  0.0669],\n         [ 0.1171,  0.2428,  0.2765, -0.1061,  0.1507,  0.1123,  0.0138,  0.2528, -0.0910,  0.2024],\n         [-0.0933,  0.2470,  0.0178, -0.0918,  0.0222, -0.0396,  0.1512,  0.2310,  0.0195,  0.1205],\n         [ 0.1871,  0.2252, -0.1103, -0.1024,  0.0265, -0.0765,  0.1559,  0.0525,  0.0626,  0.2296],\n         [-0.0404,  0.0415, -0.0964,  0.1770,  0.2407, -0.0885,  0.1886,  0.0988,  0.1241, -0.0197]]),\n tensor([[ 0.2503,  0.1817,  0.0126,  0.2533,  0.1894,  0.0409,  0.0346,  0.1906,  0.0028, -0.0500],\n         [ 0.2727,  0.1456, -0.0822,  0.2615,  0.1846, -0.0088, -0.0702,  0.0786,  0.0612,  0.0029],\n         [-0.0630,  0.1806,  0.0933,  0.0588,  0.1733,  0.1716,  0.2244, -0.0117,  0.2667,  0.1549],\n         [ 0.1669, -0.0602, -0.0441,  0.1185, -0.0435, -0.0968,  0.0763, -0.0793,  0.1789,  0.2125],\n         [-0.0753,  0.2657,  0.1949,  0.1723, -0.0889,  0.0771,  0.1567,  0.1438,  0.1492, -0.0557],\n         [-0.0700, -0.0379,  0.1252,  0.1553,  0.2358,  0.1180,  0.1481, -0.0947, -0.0519, -0.0530],\n         [-0.0951,  0.1704,  0.1018,  0.0124, -0.0813,  0.1127,  0.0923,  0.2073,  0.1874, -0.0929],\n         [ 0.0547,  0.0722,  0.1718, -0.0851, -0.0137,  0.2170,  0.2518,  0.0099, -0.0562, -0.0901],\n         [-0.0364,  0.1456,  0.0073, -0.0689, -0.0381,  0.0609,  0.1470,  0.1278,  0.1969,  0.2251],\n         [ 0.2415, -0.0258,  0.2668,  0.2486, -0.1069,  0.1885,  0.0146,  0.1928,  0.0357,  0.0407]]),\n tensor([[ 0.1598],\n         [ 1.9081],\n         [ 1.9844],\n         [ 2.6781],\n         [ 1.3237],\n         [ 1.2417],\n         [ 2.5576],\n         [ 0.4801],\n         [-0.8827],\n         [ 2.3167]])]\n\n\n\ncon = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\ncon\n\n[tensor(0.0139), tensor(-0.0015), tensor(-0.0319)]\n\n\n\nfor i in ly + con:\n    print(i)\n\ntensor([[-0.0885, -0.0638,  0.0772, -0.0345,  0.0909,  0.1749,  0.0677,  0.0966,  0.1951,  0.2063],\n        [-0.0830,  0.1576,  0.0236, -0.0339, -0.0435,  0.1856,  0.0527, -0.0925,  0.2777,  0.2286],\n        [ 0.2143,  0.2019, -0.1154, -0.1139,  0.1705,  0.1669,  0.0929,  0.0492,  0.0093,  0.0219],\n        [ 0.2064,  0.0697,  0.1346,  0.0099,  0.0619,  0.1576,  0.1706,  0.2313,  0.1813,  0.1084],\n        [-0.0234,  0.1411,  0.1117,  0.2572,  0.1412,  0.0425, -0.0518,  0.2002,  0.2679,  0.1291],\n        [ 0.0778, -0.0274, -0.0914,  0.0320,  0.0827,  0.2084, -0.0432, -0.0596, -0.1159,  0.1561],\n        [-0.0663,  0.0115, -0.0512, -0.0736,  0.1799, -0.1023,  0.2382,  0.1505, -0.1048, -0.1080],\n        [ 0.0528,  0.1592,  0.1679,  0.1505,  0.2484, -0.0191, -0.0484,  0.1934,  0.1189,  0.0669],\n        [ 0.1171,  0.2428,  0.2765, -0.1061,  0.1507,  0.1123,  0.0138,  0.2528, -0.0910,  0.2024],\n        [-0.0933,  0.2470,  0.0178, -0.0918,  0.0222, -0.0396,  0.1512,  0.2310,  0.0195,  0.1205],\n        [ 0.1871,  0.2252, -0.1103, -0.1024,  0.0265, -0.0765,  0.1559,  0.0525,  0.0626,  0.2296],\n        [-0.0404,  0.0415, -0.0964,  0.1770,  0.2407, -0.0885,  0.1886,  0.0988,  0.1241, -0.0197]])\ntensor([[ 0.2503,  0.1817,  0.0126,  0.2533,  0.1894,  0.0409,  0.0346,  0.1906,  0.0028, -0.0500],\n        [ 0.2727,  0.1456, -0.0822,  0.2615,  0.1846, -0.0088, -0.0702,  0.0786,  0.0612,  0.0029],\n        [-0.0630,  0.1806,  0.0933,  0.0588,  0.1733,  0.1716,  0.2244, -0.0117,  0.2667,  0.1549],\n        [ 0.1669, -0.0602, -0.0441,  0.1185, -0.0435, -0.0968,  0.0763, -0.0793,  0.1789,  0.2125],\n        [-0.0753,  0.2657,  0.1949,  0.1723, -0.0889,  0.0771,  0.1567,  0.1438,  0.1492, -0.0557],\n        [-0.0700, -0.0379,  0.1252,  0.1553,  0.2358,  0.1180,  0.1481, -0.0947, -0.0519, -0.0530],\n        [-0.0951,  0.1704,  0.1018,  0.0124, -0.0813,  0.1127,  0.0923,  0.2073,  0.1874, -0.0929],\n        [ 0.0547,  0.0722,  0.1718, -0.0851, -0.0137,  0.2170,  0.2518,  0.0099, -0.0562, -0.0901],\n        [-0.0364,  0.1456,  0.0073, -0.0689, -0.0381,  0.0609,  0.1470,  0.1278,  0.1969,  0.2251],\n        [ 0.2415, -0.0258,  0.2668,  0.2486, -0.1069,  0.1885,  0.0146,  0.1928,  0.0357,  0.0407]])\ntensor([[ 0.1598],\n        [ 1.9081],\n        [ 1.9844],\n        [ 2.6781],\n        [ 1.3237],\n        [ 1.2417],\n        [ 2.5576],\n        [ 0.4801],\n        [-0.8827],\n        [ 2.3167]])\ntensor(0.0139)\ntensor(-0.0015)\ntensor(-0.0319)\n\n\n\ndef init_coeffs():\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [1]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n    # Creates n-1 number of layers\n    # First layer [n_coeffs,hidden] second layer [hidden,hidden], third [hidden,1]\n    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\n    # Create (n-1) number of constants for each layer\n    for l in layers+consts: l.requires_grad_()\n    # adds requires grad to each layer and const \n    return layers,consts\n\nYou’ll notice here that there’s a lot of messy constants to get the random numbers in just the right ranges. When you train the model in a moment, you’ll see that the tiniest changes to these initialisations can cause our model to fail to train at all! This is a key reason that deep learning failed to make much progress in the early days – it’s very finicky to get a good starting point for our coefficients. Nowadays, we have ways to deal with that, which we’ll learn about in other notebooks.\nOur deep learning calc_preds looks much the same as before, but now we loop through each layer, instead of listing them separately:\n\nimport torch.nn.functional as F\n\ndef calc_preds(coeffs, indeps):\n    layers,consts = coeffs\n    n = len(layers)\n    res = indeps\n    for i,l in enumerate(layers):\n        # This does the same thing neural network does\n        # res * l_n + consts\n        # loops -> n+1\n        res = res@l + consts[i]\n        if i!=n-1: res = F.relu(res) # Updates res\n    return torch.sigmoid(res)\n\nWe also need a minor update to update_coeffs since we’ve got layers and consts separated now:\nOld ones for the comparason\ndef update_coeffs(coeffs, lr):\n    for layer in coeffs:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\ndef update_coeffs(coeffs, lr):\n    layers,consts = coeffs\n    for layer in layers+consts:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\nLet’s train our model…\n\ncoeffs = train_model(lr=4)\n\n0.521; 0.483; 0.427; 0.379; 0.379; 0.379; 0.379; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.377; 0.376; 0.371; 0.333; 0.239; 0.224; 0.208; 0.204; 0.203; 0.203; 0.207; 0.197; 0.196; 0.195; \n\n\n…and check its accuracy:\n\nacc(coeffs)\n\ntensor(0.8258)"
  },
  {
    "objectID": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html#final-thoughts",
    "href": "Projects/fastai22/part1/05-linear-model-and-neural-net-from-scratch.html#final-thoughts",
    "title": "p1-05 Linear model and Neural Net from scratch",
    "section": "Final thoughts",
    "text": "Final thoughts\nIt’s actually pretty cool that we’ve managed to create a real deep learning model from scratch and trained it to get over 80% accuracy on this task, all in the course of a single notebook!\nThe “real” deep learning models that are used in research and industry look very similar to this, and in fact if you look inside the source code of any deep learning model you’ll recognise the basic steps are the same.\nThe biggest differences in practical models to what we have above are:\n\nHow initialisation and normalisation is done to ensure the model trains correctly every time\nRegularization (to avoid over-fitting)\nModifying the neural net itself to take advantage of knowledge of the problem domain\nDoing gradient descent steps on smaller batches, rather than the whole dataset.\n\nI’ll be adding notebooks about all these later, and will add links here once they’re ready.\nIf you found this notebook useful, please remember to click the little up-arrow at the top to upvote it, since I like to know when people have found my work useful, and it helps others find it too. (BTW, be sure you’re looking at my original notebook here when you do that, and are not on your own copy of it, otherwise your upvote won’t get counted!) And if you have any questions or comments, please pop them below – I read every comment I receive!"
  },
  {
    "objectID": "Projects/fastai22/part2/matmul.html",
    "href": "Projects/fastai22/part2/matmul.html",
    "title": "p2-01 Matrix Multiplication From Foundation.",
    "section": "",
    "text": "The foundations we’ll assume throughout this course are:\n\nPython\nmatplotlib\nThe Python standard library\nJupyter notebooks and nbdev\n\n\nfrom pathlib import Path\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt"
  },
  {
    "objectID": "Projects/fastai22/part2/matmul.html#get-data",
    "href": "Projects/fastai22/part2/matmul.html#get-data",
    "title": "p2-01 Matrix Multiplication From Foundation.",
    "section": "Get data",
    "text": "Get data\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n\nurlretrieve - (read the docs!)\n\nfrom urllib.request import urlretrieve\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n\n\n!ls -l data\n\ntotal 16656\n-rw-r--r-- 1 root root 17051982 Mar 19 19:33 mnist.pkl.gz\n\n\nReading in the data from .pickle file\n\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n\nSince we are only using Python and data comes as a numpy array we will convert it to a list.\n\nlst1 = list(x_train[0])\nvals = lst1[200:210]\nvals\n\n[0.0,\n 0.0,\n 0.0,\n 0.19140625,\n 0.9296875,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125]\n\n\n\nlen(lst1)\n\n784\n\n\nNow our images are stored as 784 long list, we want it to be 28 by 28 list.\nThat’s why we create chunks function.\n\ndef my_chunks(x,sz):\n    return [x[i:i+sz] for i in range(0, len(x), sz)]\n\n\ndef chunks(x, sz):\n    for i in range(0, len(x), sz): yield x[i:i+sz]\n\nMy way to create a chunk is little slower but doesn’t use yield method.\nyield method returns a generator object which can be used to iterate over the values produced by the function.\n\n\n\n1.28 µs ± 516 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n940 ns ± 457 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n\n\n\nlist(chunks(vals,5)),my_chunks(vals,5)\n\n([[0.0, 0.0, 0.0, 0.19140625, 0.9296875],\n  [0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]],\n [[0.0, 0.0, 0.0, 0.19140625, 0.9296875],\n  [0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]])\n\n\n\nmpl.rcParams['image.cmap'] = 'gray'\nplt.imshow(list(chunks(lst1, 28)));\n\n\n\n\nislice\n\nfrom itertools import islice\n\n\nit = iter(vals)\nislice(it, 5)\n\n<itertools.islice at 0x7efbf01219a0>\n\n\nReturns “sliced” iterator that generates the specified slice of the original iterable.\n\nlist(islice(it, 5))\n\n[0.0, 0.0, 0.0, 0.19140625, 0.9296875]\n\n\nCalled again on the same iterable shows another “slice”\n\nlist(islice(it, 5))\n\n[0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]\n\n\n\nlist(islice(it, 5))\n\n[]\n\n\n\nit = iter(lst1)\nimg = list(iter(lambda: list(islice(it, 28)), []))\n\nit -> iterator of images\nimg -> returns another slice from it\n\nplt.imshow(img);"
  },
  {
    "objectID": "Projects/fastai22/part2/matmul.html#matrix-and-tensor",
    "href": "Projects/fastai22/part2/matmul.html#matrix-and-tensor",
    "title": "p2-01 Matrix Multiplication From Foundation.",
    "section": "Matrix and tensor",
    "text": "Matrix and tensor\n\nimg[20][15]\n\n0.98828125\n\n\nBut we what to index our Matrix matrix[20,15]\nSo lets create a class that allows us to do so\n\nclass Matrix:\n    def __init__(self, xs): self.xs = xs\n    def __getitem__(self, idxs): return self.xs[idxs[0]][idxs[1]]\n\n\nm = Matrix(img)\nm[20,15]\n\n0.98828125\n\n\n[Tensor Unlocked 🌟]\n\nimport torch\nfrom torch import tensor\n\n\ntensor([1,2,3])\n\ntensor([1, 2, 3])\n\n\nWe can use map to convert our data to tensor type\n\nx_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\nx_train.shape\n\ntorch.Size([50000, 784])\n\n\n\nx_train.type()\n\n'torch.FloatTensor'\n\n\nTensor\n-1 could be replace by 50000, but -1 just means fill all the data\n\nimgs = x_train.reshape((-1,28,28))\nimgs.shape\n\ntorch.Size([50000, 28, 28])\n\n\n\nplt.imshow(imgs[0]);\n\n\n\n\n\nimgs[0,20,15]\n\ntensor(0.9883)\n\n\n\nx_train.shape\n\ntorch.Size([50000, 784])\n\n\n\nn,c = x_train.shape\ny_train, y_train.shape\n\n(tensor([5, 0, 4,  ..., 8, 4, 8]), torch.Size([50000]))\n\n\n\nmin(y_train),max(y_train)\n\n(tensor(0), tensor(9))\n\n\n\ny_train.min(), y_train.max()\n\n(tensor(0), tensor(9))"
  },
  {
    "objectID": "Projects/fastai22/part2/matmul.html#random-numbers",
    "href": "Projects/fastai22/part2/matmul.html#random-numbers",
    "title": "p2-01 Matrix Multiplication From Foundation.",
    "section": "Random numbers",
    "text": "Random numbers\nBased on the Wichmann Hill algorithm used before Python 2.3.\n\nrnd_state = None\ndef seed(a):\n    global rnd_state\n    a, x = divmod(a, 30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x)+1, int(y)+1, int(z)+1\n\n\nseed(9136644127346)\nrnd_state\n\n(8675, 10445, 9961)\n\n\n\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x,y,z\n    return (x/30269 + y/30307 + z/30323) % 1.0\n\n\nrand(),rand(),rand()\n\n(0.13052416218830465, 0.7532777742751993, 0.08643536352946324)\n\n\n\nif os.fork(): print(f'In parent: {rand()}')\nelse:\n    print(f'In child: {rand()}')\n    os._exit(os.EX_OK)\n\nIn parent: 0.3403345321405016\nIn child: 0.3403345321405016\n\n\n\nif os.fork(): print(f'In parent: {torch.rand(1)}')\nelse:\n    print(f'In child: {torch.rand(1)}')\n    os._exit(os.EX_OK)\n\nIn parent: tensor([0.8574])\nIn child: tensor([0.8574])\n\n\n\nimport random\nif os.fork(): print(f'In parent: {random.random()}')\nelse:\n    print(f'In child: {random.random()}')\n    os._exit(os.EX_OK)\n\nIn parent: 0.6107283594751433\nIn child: 0.9296917080075688\n\n\n\nplt.plot([rand() for _ in range(50)]);\n\n\n\n\n\nplt.hist([rand() for _ in range(10000)]);\n\n\n\n\n\n\n\n8.57 ms ± 1.31 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nAs we can see pytorch rand version is much faster\n\n\n\nThe slowest run took 6.66 times longer than the fastest. This could mean that an intermediate result is being cached.\n106 µs ± 99 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "Projects/fastai22/part2/matmul.html#matrix-multiplication",
    "href": "Projects/fastai22/part2/matmul.html#matrix-multiplication",
    "title": "p2-01 Matrix Multiplication From Foundation.",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\ntorch.manual_seed(1)\nweights = torch.randn(784,10)\nbias = torch.zeros(10)\n\n\nweights[:5],bias\n\n(tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n          -0.7121,  0.3037],\n         [-0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676, -0.6970, -1.1608,\n           0.6995,  0.1991],\n         [ 0.8657,  0.2444, -0.6629,  0.8073,  1.1017, -0.1759, -2.2456, -1.4465,\n           0.0612, -0.6177],\n         [-0.7981, -0.1316,  1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,\n           0.1530, -0.4757],\n         [-0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n          -0.9274,  0.5451]]),\n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n\n\n\nm1 = x_valid[:5]\nm2 = weights\n\nLets grab 5 images from the validation set. m1 - is our image m2 - are our weights\n\nm1.shape,m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\n\nar,ac = m1.shape # n_rows * n_cols\nbr,bc = m2.shape\n(ar,ac),(br,bc)\n\n((5, 784), (784, 10))\n\n\n\nt1 = torch.zeros(ar, bc)\nt1.shape\n\ntorch.Size([5, 10])\n\n\nThis is dot product of m1 and m2 tensors\n\nrow = -1\ncolumn = 0\nt1[:,row],t1[column,:]\n\n(tensor([0., 0., 0., 0., 0.]),\n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n\n\nDoing a dot product:\n\nfor i in range(ar):         # 5\n    for j in range(bc):     # 10\n        for k in range(ac): # 784 \n            t1[i,j] += m1[i,k] * m2[k,j]\n\nHere is the result:\n\nt1\n\ntensor([[-10.9417,  -0.6844,  -7.0038,  -4.0066,  -2.0857,  -3.3588,   3.9127,\n          -3.4375, -11.4696,  -2.1153],\n        [ 14.5430,   5.9977,   2.8914,  -4.0777,   6.5914, -14.7383,  -9.2787,\n           2.1577, -15.2772,  -2.6758],\n        [  2.2204,  -3.2171,  -4.7988,  -6.0453,  14.1661,  -8.9824,  -4.7922,\n          -5.4446, -20.6758,  13.5657],\n        [ -6.7097,   8.8998,  -7.4611,  -7.8966,   2.6994,  -4.7260, -11.0278,\n         -12.9776,  -6.4443,   3.6376],\n        [ -2.4444,  -6.4034,  -2.3984,  -9.0371,  11.1772,  -5.7724,  -8.9214,\n          -3.7862,  -8.9827,   5.2797]])\n\n\nAs we can see there are identical.\n\nm1 @ m2\n\ntensor([[-10.9417,  -0.6844,  -7.0038,  -4.0066,  -2.0857,  -3.3588,   3.9127,\n          -3.4375, -11.4696,  -2.1153],\n        [ 14.5430,   5.9977,   2.8914,  -4.0777,   6.5914, -14.7383,  -9.2787,\n           2.1577, -15.2772,  -2.6758],\n        [  2.2204,  -3.2171,  -4.7988,  -6.0453,  14.1661,  -8.9824,  -4.7922,\n          -5.4446, -20.6758,  13.5657],\n        [ -6.7097,   8.8998,  -7.4611,  -7.8966,   2.6994,  -4.7260, -11.0278,\n         -12.9776,  -6.4443,   3.6376],\n        [ -2.4444,  -6.4034,  -2.3984,  -9.0371,  11.1772,  -5.7724,  -8.9214,\n          -3.7862,  -8.9827,   5.2797]])\n\n\n\nt1.shape\n\ntorch.Size([5, 10])\n\n\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\nt1\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\nimport numpy as np\nnp.set_printoptions(precision=2, linewidth=140)\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n    return c\n\n\n\n\nCPU times: user 616 ms, sys: 2.94 ms, total: 619 ms\nWall time: 632 ms\n\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\nar*bc*ac\n\n39200"
  },
  {
    "objectID": "Projects/fastai22/part2/matmul.html#numba",
    "href": "Projects/fastai22/part2/matmul.html#numba",
    "title": "p2-01 Matrix Multiplication From Foundation.",
    "section": "Numba",
    "text": "Numba\n\nWe are importing njit decorator since:\n\n@njit decorator is used to optimize the function using the Numba library’s just-in-time (JIT) compilation, which can significantly improve performance.\n\nfrom numba import njit\n\n\n@njit\ndef dot(a,b):\n    res = 0.\n    for i in range(len(a)): res+=a[i]*b[i]\n    return res\n\n\nfrom numpy import array\n\n\n\n\nCPU times: user 361 ms, sys: 142 ms, total: 503 ms\nWall time: 440 ms\n\n\n20.0\n\n\n\ndef m_dot(a,b):\n    res = 0.\n    for i in range(len(a)): res+=a[i]*b[i]\n    return res\n\nAlthough it dosn’t seem faster on small scale, it speeds things up with more complex functions.\n\n\n\nCPU times: user 28 µs, sys: 2 µs, total: 30 µs\nWall time: 33.6 µs\n\n\n20.0\n\n\nNow only two of our loops are running in Python, not three:\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = dot(a[i,:], b[:,j])\n    return c\n\nnumba does not work with pytorch tensors so let’s convert them to numpy arrays\n\nm1a,m2a = m1.numpy(),m2.numpy()\n\n\nfrom fastcore.test import *\n\n\ntest_close(t1,matmul(m1a, m2a))\n\n\n\n\n455 µs ± 52.8 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nHere we can see that it is quite faster than our old matmul."
  },
  {
    "objectID": "Projects/fastai22/part2/matmul.html#elementwise-ops",
    "href": "Projects/fastai22/part2/matmul.html#elementwise-ops",
    "title": "p2-01 Matrix Multiplication From Foundation.",
    "section": "Elementwise ops",
    "text": "Elementwise ops\nTryAPL\n\na = tensor([10., 6, -4])\nb = tensor([2., 8, 7])\na,b\n\n(tensor([10.,  6., -4.]), tensor([2., 8., 7.]))\n\n\n\na + b\n\ntensor([12., 14.,  3.])\n\n\n\na<b\n\ntensor([False,  True,  True])\n\n\nPython converts True to 1 and False to 0\n\n(a < b).float().mean()\n\ntensor(0.67)\n\n\n\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]]); m\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\nFrobenius norm:\n\\[\\| A \\|_F = \\left( \\sum_{i,j=1}^n | a_{ij} |^2 \\right)^{1/2}\\]\nHint: you don’t normally need to write equations in LaTeX yourself, instead, you can click ‘edit’ in Wikipedia and copy the LaTeX from there (which is what I did for the above equation). Or on arxiv.org, click “Download: Other formats” in the top right, then “Download source”; rename the downloaded file to end in .tgz if it doesn’t already, and you should find the source there, including the equations to copy and paste. This is the source LaTeX that I pasted to render the equation above:\n$$\\| A \\|_F = \\left( \\sum_{i,j=1}^n | a_{ij} |^2 \\right)^{1/2}$$\n\nsf = (m*m).sum()\nsf\n\ntensor(285.)\n\n\n\nsf.sqrt()\n\ntensor(16.88)\n\n\n\nm[2,:],m[:,2]\n\n(tensor([7., 8., 9.]), tensor([3., 6., 9.]))\n\n\n\nm[2]\n\ntensor([7., 8., 9.])\n\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = (a[i,:] * b[:,j]).sum()\n    return c\n\n\ntest_close(t1,matmul(m1, m2))\n\n%timeit with Numba\n241 µs ± 33.6 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\n1.02 ms ± 73.6 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = torch.dot(a[i,:], b[:,j])\n    return c\n\n\ntest_close(t1,matmul(m1, m2))\n\n\n\n\n795 µs ± 91.6 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nNumba solution is still faster, but this solution is also fast and works with tensors"
  },
  {
    "objectID": "Projects/fastai22/part2/matmul.html#broadcasting",
    "href": "Projects/fastai22/part2/matmul.html#broadcasting",
    "title": "p2-01 Matrix Multiplication From Foundation.",
    "section": "Broadcasting",
    "text": "Broadcasting\nThe term broadcasting describes how arrays with different shapes are treated during arithmetic operations.\nFrom the Numpy Documentation:\nThe term broadcasting describes how numpy treats arrays with \ndifferent shapes during arithmetic operations. Subject to certain \nconstraints, the smaller array is “broadcast” across the larger \narray so that they have compatible shapes. Broadcasting provides a \nmeans of vectorizing array operations so that looping occurs in C\ninstead of Python. It does this without making needless copies of \ndata and usually leads to efficient algorithm implementations.\nIn addition to the efficiency of broadcasting, it allows developers to write less code, which typically leads to fewer errors.\nThis section was adapted from Chapter 4 of the fast.ai Computational Linear Algebra course.\n\nBroadcasting with a scalar\n\na\n\ntensor([10.,  6., -4.])\n\n\n\na > 0\n\ntensor([ True,  True, False])\n\n\nHow are we able to do a > 0? 0 is being broadcast to have the same dimensions as a.\nFor instance you can normalize our dataset by subtracting the mean (a scalar) from the entire data set (a matrix) and dividing by the standard deviation (another scalar), using broadcasting.\nOther examples of broadcasting with a scalar:\n\na + 1\n\ntensor([11.,  7., -3.])\n\n\n\nm\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\n\n2*m\n\ntensor([[ 2.,  4.,  6.],\n        [ 8., 10., 12.],\n        [14., 16., 18.]])\n\n\n\n\nBroadcasting a vector to a matrix\nAlthough broadcasting a scalar is an idea that dates back to APL, the more powerful idea of broadcasting across higher rank tensors comes from a little known language called Yorick.\nWe can also broadcast a vector to a matrix:\n\nc = tensor([10.,20,30]); c\n\ntensor([10., 20., 30.])\n\n\n\nm\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\n\nm.shape,c.shape\n\n(torch.Size([3, 3]), torch.Size([3]))\n\n\n\nm + c\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\n\nc + m\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\n\nt = c.expand_as(m)\n\n\nt\n\ntensor([[10., 20., 30.],\n        [10., 20., 30.],\n        [10., 20., 30.]])\n\n\n\nm + t\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\nWe don’t really copy the rows, but it looks as if we did. In fact, the rows are given a stride of 0.\n\nt.storage()\n\n 10.0\n 20.0\n 30.0\n[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 3]\n\n\n\nt.stride(), t.shape\n\n((0, 1), torch.Size([3, 3]))\n\n\nYou can index with the special value [None] or use unsqueeze() to convert a 1-dimensional array into a 2-dimensional array (although one of those dimensions has value 1).\n\nc\n\ntensor([10., 20., 30.])\n\n\n\nc.unsqueeze(0), c[None, :], c[:,None] # c[:,:] dosn't work\n\n(tensor([[10., 20., 30.]]), tensor([[10., 20., 30.]]), tensor([[10.],\n         [20.],\n         [30.]]))\n\n\n\nc.shape, c.unsqueeze(0).shape\n\n(torch.Size([3]), torch.Size([1, 3]))\n\n\n\nc.unsqueeze(1), c[:, None]\n\n(tensor([[10.],\n         [20.],\n         [30.]]), tensor([[10.],\n         [20.],\n         [30.]]))\n\n\n\nc.shape, c.unsqueeze(1).shape\n\n(torch.Size([3]), torch.Size([3, 1]))\n\n\nYou can always skip trailling ‘:’s. And’…’ means ‘all preceding dimensions’\n\nc[None].shape,c[...,None].shape\n\n(torch.Size([1, 3]), torch.Size([3, 1]))\n\n\n\nc[:,None].expand_as(m),\"OLD\",c.expand_as(m)\n\n(tensor([[10., 10., 10.],\n         [20., 20., 20.],\n         [30., 30., 30.]]), 'OLD', tensor([[10., 20., 30.],\n         [10., 20., 30.],\n         [10., 20., 30.]]))\n\n\n\nm + c[:,None]\n\ntensor([[11., 12., 13.],\n        [24., 25., 26.],\n        [37., 38., 39.]])\n\n\n\nm + c[None,:]\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\n\nNOTE\nC[columns,rows] indexing * C[None.:] WHERE column is none so we add on rows * C[:,None] WHERE rows is None so we add on columns\n\n\n\nBroadcasting Rules\n\nc[None,:],c[None,:].shape\n\n(tensor([[10., 20., 30.]]), torch.Size([1, 3]))\n\n\n\nc[:,None],c[:,None].shape\n\n(tensor([[10.],\n         [20.],\n         [30.]]), torch.Size([3, 1]))\n\n\n\nc[None,:] * c[:,None]\n\ntensor([[100., 200., 300.],\n        [200., 400., 600.],\n        [300., 600., 900.]])\n\n\n\nc[None] > c[:,None]\n\ntensor([[False,  True,  True],\n        [False, False,  True],\n        [False, False, False]])\n\n\n\nm*m\n\ntensor([[ 1.,  4.,  9.],\n        [16., 25., 36.],\n        [49., 64., 81.]])\n\n\nWhen operating on two arrays/tensors, Numpy/PyTorch compares their shapes element-wise. It starts with the trailing dimensions, and works its way forward. Two dimensions are compatible when\n\nthey are equal, or\none of them is 1, in which case that dimension is broadcasted to make it the same size\n\nArrays do not need to have the same number of dimensions. For example, if you have a 256*256*3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:\nImage  (3d array): 256 x 256 x 3\nScale  (1d array):             3\nResult (3d array): 256 x 256 x 3\nThe numpy documentation includes several examples of what dimensions can and can not be broadcast together.\n\nTEST_1, TEST_2 = torch.tensor([[2,2,2],[2,2,2]]), torch.tensor([[3,3,3],[3,3,3],[3,3,3]])\ntry: \n    TEST_1 * TEST_2\nexcept:\n    print(\"This will return an error since the dimensions are not compatible\")\n\nThis will return an error since the dimensions are not compatible"
  },
  {
    "objectID": "Projects/fastai22/part2/matmul.html#matmul-with-broadcasting",
    "href": "Projects/fastai22/part2/matmul.html#matmul-with-broadcasting",
    "title": "p2-01 Matrix Multiplication From Foundation.",
    "section": "Matmul with broadcasting",
    "text": "Matmul with broadcasting\n\ndigit = m1[0]\ndigit.shape,m2.shape\n\n(torch.Size([784]), torch.Size([784, 10]))\n\n\n\ndigit[:,None].shape\n\ntorch.Size([784, 1])\n\n\nWe need to ‘reshape’ our digit so it is possible to multiply by our weights.\n\ndigit[:,None].expand_as(m2).shape\n\ntorch.Size([784, 10])\n\n\nThe shape of our output.\n\n(digit[:,None]*m2).shape\n\ntorch.Size([784, 10])\n\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n#       c[i,j] = (a[i,:] * b[:,j]).sum()      # previous version\n        c[i]   = (a[i,:,None] * b).sum(dim=0) # broadcast version\n    return c\n\n\ntest_close(t1,matmul(m1, m2))\n\n\n\n\n135 µs ± 27.9 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nOur time has gone from ~500ms to <0.1ms, an over 5000x improvement! We can run on the whole dataset now.\n\ntr = matmul(x_train, weights)\ntr\n\ntensor([[  0.96,  -2.96,  -2.11,  ..., -15.09, -17.69,   0.60],\n        [  6.89,  -0.34,   0.79,  ..., -17.13, -25.36,  16.23],\n        [-10.18,   7.38,   4.13,  ...,  -6.73,  -6.79,  -1.58],\n        ...,\n        [  7.40,   7.64,  -3.50,  ...,  -1.02, -16.22,   2.07],\n        [  3.25,   9.52,  -9.37,  ...,   2.98, -19.58,  -1.96],\n        [ 15.70,   4.12,  -5.62,  ...,   8.08, -12.21,   0.42]])\n\n\n\ntr.shape\n\ntorch.Size([50000, 10])\n\n\n\n\n\nCPU times: user 1.22 s, sys: 9.29 ms, total: 1.23 s\nWall time: 1.24 s"
  },
  {
    "objectID": "Projects/fastai22/part2/matmul.html#einstein-summation",
    "href": "Projects/fastai22/part2/matmul.html#einstein-summation",
    "title": "p2-01 Matrix Multiplication From Foundation.",
    "section": "Einstein summation",
    "text": "Einstein summation\nEinstein summation (einsum) is a compact representation for combining products and sums in a general way. The key rules are:\n\nRepeating letters between input arrays means that values along those axes will be multiplied together.\nOmitting a letter from the output means that values along that axis will be summed.\n\n\nm1.shape,m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\nThe notation ik,kj->ij is used to perform matrix multiplication between two matrices m1 and m2.\nThe first part of the notation ik refers to the dimensions of the first matrix m1. The second part of the notation kj refers to the dimensions of the second matrix m2.\nThe last part of the notation ij refers to the dimensions of the output matrix mr. The dimensions of the output matrix mr are determined by the dimensions of the input matrices m1 and m2.\nWhere i and j are the dimensions of the input matrices, and k is the common dimension of the two matrices.\n\ntorch.einsum('ik,kj -> ij',m1,m2).shape,torch.einsum('ik,kj -> ikj',m1,m2).shape\n\n(torch.Size([5, 10]), torch.Size([5, 784, 10]))\n\n\n\n# c[i,j] += a[i,k] * b[k,j]\n# c[i,j] = (a[i,:] * b[:,j]).sum()\nmr = torch.einsum('ik,kj->ikj', m1, m2)\nmr.shape\n\ntorch.Size([5, 784, 10])\n\n\n\nmr.sum(1)\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\ntorch.einsum('ik,kj->ij', m1, m2)\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\ndef matmul(a,b): return torch.einsum('ik,kj->ij', a, b)\n\n\ntest_close(tr, matmul(x_train, weights), eps=1e-3)\n\n\n\n\n21.8 ms ± 490 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)"
  },
  {
    "objectID": "Projects/fastai22/part2/matmul.html#pytorch-op",
    "href": "Projects/fastai22/part2/matmul.html#pytorch-op",
    "title": "p2-01 Matrix Multiplication From Foundation.",
    "section": "pytorch op",
    "text": "pytorch op\nWe can use pytorch’s function or operator directly for matrix multiplication.\n\ntest_close(tr, x_train@weights, eps=1e-3)\n\n\n\n\n21.2 ms ± 1.17 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)"
  },
  {
    "objectID": "Projects/fastai22/part2/matmul.html#cuda",
    "href": "Projects/fastai22/part2/matmul.html#cuda",
    "title": "p2-01 Matrix Multiplication From Foundation.",
    "section": "CUDA",
    "text": "CUDA\n\ndef matmul(grid, a,b,c):\n    i,j = grid\n    if i < c.shape[0] and j < c.shape[1]:\n        tmp = 0.\n        for k in range(a.shape[1]): tmp += a[i, k] * b[k, j]\n        c[i,j] = tmp\n\n\nres = torch.zeros(ar, bc)\nmatmul((1,1), m1, m2, res)\nres\n\ntensor([[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n        [0.00, 6.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n        [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n        [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n        [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]])\n\n\n\ndef launch_kernel(kernel, grid_x, grid_y, *args, **kwargs):\n    for i in range(grid_x):\n        for j in range(grid_y): kernel((i,j), *args, **kwargs)\n\n\nres = torch.zeros(ar, bc)\nlaunch_kernel(matmul, ar, bc, m1, m2, res)\nres\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\nfrom numba import cuda\n\n\ndef matmul(grid, a,b,c):\n    i,j = grid\n    if i < c.shape[0] and j < c.shape[1]:\n        tmp = 0.\n        for k in range(a.shape[1]): tmp += a[i, k] * b[k, j]\n        c[i,j] = tmp\n\n\n@cuda.jit\ndef matmul(a,b,c):\n    i, j = cuda.grid(2)\n    if i < c.shape[0] and j < c.shape[1]:\n        tmp = 0.\n        for k in range(a.shape[1]): tmp += a[i, k] * b[k, j]\n        c[i,j] = tmp\n\nHere we are “Sending” our weights and images to the GPU\n\nr = np.zeros(tr.shape)\nm1g,m2g,rg = map(cuda.to_device, (x_train,weights,r))\n\n\nr.shape\n\n(50000, 10)\n\n\n\nTPB = 16\nrr,rc = r.shape\nblockspergrid = (math.ceil(rr / TPB), math.ceil(rc / TPB))\nblockspergrid\n\n(3125, 1)\n\n\n\nmatmul[blockspergrid, (TPB,TPB)](m1g,m2g,rg)\nr = rg.copy_to_host()\ntest_close(tr, r, eps=1e-3)\n\n\nmatmul[blockspergrid, (TPB,TPB)](m1g,m2g,rg)\nr = rg.copy_to_host()\n\n11.3 ms ± 4.02 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nm1c,m2c = x_train.cuda(),weights.cuda()\n\n\nr=(m1c@m2c).cpu()\n\n\n\n\n2.21 ms ± 361 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nOur broadcasting version was >500ms, and our CUDA version is around 0.5ms, which is another 1000x improvement compared to broadcasting. So our total speedup is around 5 million times!"
  },
  {
    "objectID": "Projects/fastai22/part2/meanshift.html",
    "href": "Projects/fastai22/part2/meanshift.html",
    "title": "p2-02_meanshift",
    "section": "",
    "text": "Clustering techniques are unsupervised learning algorithms that try to group unlabelled data into “clusters”, using the (typically spatial) structure of the data itself. It has many applications.\nThe easiest way to demonstrate how clustering works is to simply generate some data and show them in action. We’ll start by importing the libraries we’ll be using today.\n\nimport math, matplotlib.pyplot as plt, operator, torch\nfrom functools import partial\n\n\ntorch.manual_seed(42)\ntorch.set_printoptions(precision=3, linewidth=140, sci_mode=False)\n\n\n\n\nn_clusters= 6\nn_samples =250\n\nTo generate our data, we’re going to pick 6 random points, which we’ll call centroids, and for each point we’re going to generate 250 random points about it.\n\ncentroids = torch.rand(n_clusters, 2)*70-35\n\n\ncentroids\n\ntensor([[ 26.759,  29.050],\n        [ -8.200,  32.151],\n        [ -7.669,   7.063],\n        [-17.040,  20.555],\n        [ 30.854, -25.677],\n        [ 30.422,   6.551]])\n\n\n\nfrom torch.distributions.multivariate_normal import MultivariateNormal\nfrom torch import tensor\n\n\ndef sample(m): return MultivariateNormal(m, torch.diag(tensor([5.,5.]))).sample((n_samples,))\n\n\nslices = [sample(c) for c in centroids]\n\ntorch.cat - concatenates given sequence.\ntorch.cat() can be seen as an inverse operation for torch.split() and torch.chunk().\n\ndata = torch.cat(slices)\ndata.shape\n\ntorch.Size([1500, 2])\n\n\nBelow we can see each centroid marked w/ X, and the coloring associated to each respective cluster.\n\ndef plot_data(centroids, data, n_samples, ax=None):\n    if ax is None: _,ax = plt.subplots()\n    for i, centroid in enumerate(centroids):\n        samples = data[i*n_samples:(i+1)*n_samples]\n        ax.scatter(samples[:,0], samples[:,1], s=1)\n        ax.plot(*centroid, markersize=10, marker=\"x\", color='k', mew=5)\n        ax.plot(*centroid, markersize=5, marker=\"x\", color='m', mew=2)\n\n\nplot_data(centroids, data, n_samples)\n\n\n\n\n\n\n\nMost people that have come across clustering algorithms have learnt about k-means. Mean shift clustering is a newer and less well-known approach, but it has some important advantages: * It doesn’t require selecting the number of clusters in advance, but instead just requires a bandwidth to be specified, which can be easily chosen automatically * It can handle clusters of any shape, whereas k-means (without using special extensions) requires that clusters be roughly ball shaped.\nThe algorithm is as follows: * For each data point x in the sample X, find the distance between that point x and every other point in X * Create weights for each point in X by using the Gaussian kernel of that point’s distance to x * This weighting approach penalizes points further away from x * The rate at which the weights fall to zero is determined by the bandwidth, which is the standard deviation of the Gaussian * Update x as the weighted average of all other points in X, weighted based on the previous step\nThis will iteratively push points that are close together even closer until they are next to each other.\n\nmidp = data.mean(0)\nmidp\n\ntensor([ 9.222, 11.604])\n\n\n\nplot_data([midp]*6, data, n_samples)\n\n\n\n\nSo here’s the definition of the gaussian kernel, which you may remember from high school… This person at the science march certainly remembered!\n\n\ndef gaussian(d, bw): return torch.exp(-0.5*((d/bw))**2) / (bw*math.sqrt(2*math.pi))\n\ntroch.linespace(start,end,step) - Creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end, inclusive * start (float) – the starting value for the set of points * end (float) – the ending value for the set of points * steps (int) – size of the constructed tensor\n\ntorch.linspace(0,10,11)\n\ntensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n\n\n\ndef plot_func(f):\n    x = torch.linspace(0,10,100)\n    plt.plot(x, f(x))\n\n\nplot_func(partial(gaussian, bw=2.5))\n\n\n\n\nthe partial function is a higher-order function that allows you to create a new function by “partial” application of an existing function. In other words, it lets you fix some of the arguments of a function and create a new function with fewer arguments.\n\npartial\n\nfunctools.partial\n\n\nIn our implementation, we choose the bandwidth to be 2.5.\nOne easy way to choose bandwidth is to find which bandwidth covers one third of the data.\n\ndef tri(d, i): return (-d+i).clamp_min(0)/i\n\n\nplot_func(partial(tri, i=8))\n\n\n\n\n\nX = data.clone()\nx = data[0]\n\n\nx,X[0]\n\n(tensor([26.204, 26.349]), tensor([26.204, 26.349]))\n\n\n\nx.shape, X.shape, x[None].shape\n\n(torch.Size([2]), torch.Size([1500, 2]), torch.Size([1, 2]))\n\n\n\n(x[None]-X)[:8]\n\ntensor([[ 0.000,  0.000],\n        [ 0.513, -3.865],\n        [-4.227, -2.345],\n        [ 0.557, -3.685],\n        [-5.033, -3.745],\n        [-4.073, -0.638],\n        [-3.415, -5.601],\n        [-1.920, -5.686]])\n\n\n\n(x-X)[:8].shape\n\ntorch.Size([8, 2])\n\n\n\n(x-X)**2\n\ntensor([[    0.000,     0.000],\n        [    0.263,    14.936],\n        [   17.871,     5.498],\n        ...,\n        [   20.869,   289.860],\n        [    9.926,   501.266],\n        [   24.641,   442.675]])\n\n\n\n# rewrite using torch.einsum\nfrom torch import einsum\nd = torch.einsum('ij,ij -> i',x-X,x-X).sqrt()\nd[:8]\n\ntensor([0.000, 3.899, 4.834, 3.726, 6.273, 4.122, 6.560, 6.002])\n\n\n\ndist = ((x-X)**2).sum(1).sqrt()\ndist[:8]\n\ntensor([0.000, 3.899, 4.834, 3.726, 6.273, 4.122, 6.560, 6.002])\n\n\n\nweight = gaussian(dist, 2.5)\nweight\n\ntensor([    0.160,     0.047,     0.025,  ...,     0.000,     0.000,     0.000])\n\n\n\nweight.shape,X.shape\n\n(torch.Size([1500]), torch.Size([1500, 2]))\n\n\n\nweight[:,None].shape\n\ntorch.Size([1500, 1])\n\n\n\nweight[:,None]*X\n\ntensor([[    4.182,     4.205],\n        [    1.215,     1.429],\n        [    0.749,     0.706],\n        ...,\n        [    0.000,     0.000],\n        [    0.000,     0.000],\n        [    0.000,     0.000]])\n\n\n\ndef one_update(X):\n    for i, x in enumerate(X):\n        # Here we calculate the distance for each point x\n        #dist = torch.einsum('ij,ij -> i',x-X,x-X).sqrt() # IS SLOWER\n        dist = torch.sqrt(((x-X)**2).sum(1)) # OLD FORMULA\n#         weight = gaussian(dist, 2.5)\n        weight = tri(dist, 8)\n        X[i] = (weight[:,None]*X).sum(0)/weight.sum()\n\n\ndef meanshift(data):\n    X = data.clone()\n    for it in range(5): one_update(X)\n    return X\n\n\n\n\nCPU times: user 1.07 s, sys: 11.6 ms, total: 1.08 s\nWall time: 816 ms\n\n\n\nplot_data(centroids+2, X, n_samples)\n\n\n\n\n\n\n\n\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML\n\n\ndef do_one(d):\n    if d: one_update(X)\n    ax.clear()\n    plot_data(centroids+2, X, n_samples, ax=ax)\n\n\n# create your animation\nX = data.clone()\nfig,ax = plt.subplots()\nani = FuncAnimation(fig, do_one, frames=5, interval=300,repeat=False)\nplt.close()\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nplot_data(centroids+2, data, n_samples)\n\n\n\n\n\n\n\nTo truly accelerate the algorithm, we need to be performing updates on a batch of points per iteration, instead of just one as we were doing.\n\nbs=5 #5 # Will try later on n_batch_cluser ~~ 6\nX = data.clone()\nx = X[:bs]\nx.shape,X.shape\n\n(torch.Size([5, 2]), torch.Size([1500, 2]))\n\n\n\nX[-6:,None]\n\ntensor([[[30.164,  9.484]],\n\n        [[29.829, 10.060]],\n\n        [[29.688,  2.965]],\n\n        [[30.772,  9.323]],\n\n        [[29.354,  3.960]],\n\n        [[31.168,  5.309]]])\n\n\n\n(x[None] - X[-6:,None]).sum(2).shape\n\ntorch.Size([6, 5])\n\n\n\ndef dist_b(a,b): return (((a[None] - b[:,None])**2).sum(2)).sqrt()\n\n\ndist_b(X, x)\n\ntensor([[ 0.000,  3.899,  4.834,  ..., 17.628, 22.610, 21.617],\n        [ 3.899,  0.000,  4.978,  ..., 21.499, 26.508, 25.500],\n        [ 4.834,  4.978,  0.000,  ..., 19.373, 24.757, 23.396],\n        [ 3.726,  0.185,  4.969,  ..., 21.335, 26.336, 25.333],\n        [ 6.273,  5.547,  1.615,  ..., 20.775, 26.201, 24.785]])\n\n\n\ndist_b(X, x).shape\n\ntorch.Size([5, 1500])\n\n\n\nX[None,:]\n\ntensor([[[26.204, 26.349],\n         [25.691, 30.213],\n         [30.431, 28.693],\n         ...,\n         [30.772,  9.323],\n         [29.354,  3.960],\n         [31.168,  5.309]]])\n\n\n\nX[None,:].shape, x[:,None].shape, (X[None,:]-x[:,None]).shape\n\n(torch.Size([1, 1500, 2]), torch.Size([5, 1, 2]), torch.Size([5, 1500, 2]))\n\n\n\nweight = gaussian(dist_b(X, x), 2)\nweight\n\ntensor([[    0.199,     0.030,     0.011,  ...,     0.000,     0.000,     0.000],\n        [    0.030,     0.199,     0.009,  ...,     0.000,     0.000,     0.000],\n        [    0.011,     0.009,     0.199,  ...,     0.000,     0.000,     0.000],\n        [    0.035,     0.199,     0.009,  ...,     0.000,     0.000,     0.000],\n        [    0.001,     0.004,     0.144,  ...,     0.000,     0.000,     0.000]])\n\n\n\nweight.shape,X.shape\n\n(torch.Size([5, 1500]), torch.Size([1500, 2]))\n\n\n\nweight[:,None].shape, X[None].shape\n\n(torch.Size([5, 1, 1500]), torch.Size([1, 1500, 2]))\n\n\n\nnum = (weight[...,None]*X[None]).sum(1)\nnum.shape\n\ntorch.Size([5, 2])\n\n\n\nnum\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.217],\n        [231.302, 234.155]])\n\n\n\ntorch.einsum('ij,jk->ik', weight, X)\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.217],\n        [231.302, 234.155]])\n\n\n\nweight@X\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.217],\n        [231.302, 234.155]])\n\n\n\ndiv = weight.sum(1, keepdim=True)\ndiv.shape\n\ntorch.Size([5, 1])\n\n\n\nnum/div\n\ntensor([[26.376, 27.692],\n        [26.101, 29.643],\n        [28.892, 28.990],\n        [26.071, 29.559],\n        [29.323, 29.685]])\n\n\n\ndef meanshift(data, bs=500, iter = 5):\n    n = len(data) # n=1500\n    X = data.clone() # X=data\n    for it in range(iter):\n        \n        for i in range(0, n, bs): #step by batch size\n            s = slice(i, min(i+bs,n))\n            weight = gaussian(dist_b(X, X[s]), 2.5)\n#             weight = tri(dist_b(X, X[s]), 8)\n            div = weight.sum(1, keepdim=True)\n            X[s] = weight@X/div\n    return X\n\nAlthough each iteration still has to launch a new cuda kernel, there are now fewer iterations, and the acceleration from updating a batch of points more than makes up for it.\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\ndata = data.cuda()\n\n\nX = meanshift(data).cpu()\n\n\ntime = %timeit -n 5 -o meanshift(data, 1250).cpu()\n\n6.49 ms ± 1.2 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\n\nX =  meanshift(data)\n\n\nX[:10],data[:10]\n\n(tensor([[26.758, 29.152],\n         [26.758, 29.152],\n         [26.758, 29.152],\n         [26.758, 29.152],\n         [26.758, 29.152],\n         [26.758, 29.152],\n         [26.758, 29.152],\n         [26.758, 29.152],\n         [26.758, 29.152],\n         [26.758, 29.152]], device='cuda:0'),\n tensor([[26.204, 26.349],\n         [25.691, 30.213],\n         [30.431, 28.693],\n         [25.647, 30.033],\n         [31.236, 30.093],\n         [30.276, 26.987],\n         [29.619, 31.949],\n         [28.124, 32.035],\n         [24.275, 29.255],\n         [21.578, 28.565]], device='cuda:0'))\n\n\nAnimation\n\ndef one_update(data,bs=500):\n    n = len(data)\n    for i in range(0,n,bs):\n        s = slice(i,min(i+bs,n))\n        weight = gaussian(dist_b(data, data[s]), 2.5)\n        div = weight.sum(1, keepdim=True)\n        X[s] = weight@X/div\n\n\nX.cpu().numpy()\n\narray([[26.757818, 29.151628],\n       [26.757818, 29.151628],\n       [26.757818, 29.151628],\n       ...,\n       [30.456202,  6.457344],\n       [30.456202,  6.457344],\n       [30.456202,  6.457344]], dtype=float32)\n\n\n\ndef do_one(d):\n    if d:one_update(X)\n    ax.clear()\n    plot_data(centroids+2, X, n_samples, ax=ax)\n\nX = data.clone()\nX = X.cpu()\nfig,ax = plt.subplots()\nani = FuncAnimation(fig, do_one, frames=5, interval=300,repeat=False)\nplt.close()\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "Project_index/forest_index.html",
    "href": "Project_index/forest_index.html",
    "title": "Forest analytics",
    "section": "",
    "text": "Input Data\n\n\n\n\n\n\n  \n    \n      \n      Start Time\n      End Time\n      Tag\n      Note\n      Tree Type\n      Is Success\n    \n  \n  \n    \n      0\n      Mon Dec 19 23:31:07 GMT+01:00 2022\n      Mon Dec 19 23:41:07 GMT+01:00 2022\n      Unset\n      NaN\n      Cedar\n      True\n    \n    \n      1\n      Wed Dec 21 02:27:55 GMT+01:00 2022\n      Wed Dec 21 03:27:20 GMT+01:00 2022\n      Unset\n      NaN\n      Cedar\n      False\n    \n    \n      2\n      Sun Feb 12 10:59:44 GMT+01:00 2023\n      Sun Feb 12 11:57:10 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n    \n    \n      3\n      Sun Feb 12 13:58:58 GMT+01:00 2023\n      Sun Feb 12 15:58:58 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n    \n    \n      4\n      Sun Feb 12 17:52:56 GMT+01:00 2023\n      Sun Feb 12 18:57:36 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n    \n  \n\n\n\n\nRemoving unsuccessfull study attempts\n\n\nCode\ndf = df[df['Is Success'] == True]\n# df = df[df['Tag'] != \"Unset\"] -> Tag u set / whether u studing or smth else / We assume u always use app for study\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      Start Time\n      End Time\n      Tag\n      Note\n      Tree Type\n      Is Success\n    \n  \n  \n    \n      0\n      Mon Dec 19 23:31:07 GMT+01:00 2022\n      Mon Dec 19 23:41:07 GMT+01:00 2022\n      Unset\n      NaN\n      Cedar\n      True\n    \n    \n      2\n      Sun Feb 12 10:59:44 GMT+01:00 2023\n      Sun Feb 12 11:57:10 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n    \n    \n      3\n      Sun Feb 12 13:58:58 GMT+01:00 2023\n      Sun Feb 12 15:58:58 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n    \n    \n      4\n      Sun Feb 12 17:52:56 GMT+01:00 2023\n      Sun Feb 12 18:57:36 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n    \n    \n      5\n      Sun Feb 12 20:53:34 GMT+01:00 2023\n      Sun Feb 12 22:53:34 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n    \n  \n\n\n\n\n\n\nTime colums\nLets create the function that calculates time difference between Start Time and End Time\nApply our function to the dataframe and create new column for calculated values\n\n\nCode\nfrom datetime import datetime\n\ndef calc_time_diff(start_time,end_time):\n    # Convert string dates to datetime objects\n    start_time = datetime.strptime(start_time, '%a %b %d %H:%M:%S %Z%z %Y')\n    end_time = datetime.strptime(end_time, '%a %b %d %H:%M:%S %Z%z %Y')\n\n    # Calculate time difference in Minutes\n    time_diff = (end_time - start_time).total_seconds()//60\n\n    return time_diff\n\ndf['Study Time'] = df.apply(lambda row: calc_time_diff(row['Start Time'],row['End Time']),axis=1)\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      Start Time\n      End Time\n      Tag\n      Note\n      Tree Type\n      Is Success\n      Study Time\n    \n  \n  \n    \n      0\n      Mon Dec 19 23:31:07 GMT+01:00 2022\n      Mon Dec 19 23:41:07 GMT+01:00 2022\n      Unset\n      NaN\n      Cedar\n      True\n      10.0\n    \n    \n      2\n      Sun Feb 12 10:59:44 GMT+01:00 2023\n      Sun Feb 12 11:57:10 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n      57.0\n    \n    \n      3\n      Sun Feb 12 13:58:58 GMT+01:00 2023\n      Sun Feb 12 15:58:58 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n      120.0\n    \n    \n      4\n      Sun Feb 12 17:52:56 GMT+01:00 2023\n      Sun Feb 12 18:57:36 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n      64.0\n    \n    \n      5\n      Sun Feb 12 20:53:34 GMT+01:00 2023\n      Sun Feb 12 22:53:34 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n      120.0\n    \n  \n\n\n\n\nWe want to group our Study Time by each day its gonna be hard with our current date format.\nSo we will create another column Study Date\n\n\nCode\ndef get_date(row):\n    return datetime.strptime(row,'%a %b %d %H:%M:%S %Z%z %Y').date()\n\ndf['Study Date'] = df.apply(lambda row: get_date(row['Start Time']),axis=1)\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      Start Time\n      End Time\n      Tag\n      Note\n      Tree Type\n      Is Success\n      Study Time\n      Study Date\n    \n  \n  \n    \n      0\n      Mon Dec 19 23:31:07 GMT+01:00 2022\n      Mon Dec 19 23:41:07 GMT+01:00 2022\n      Unset\n      NaN\n      Cedar\n      True\n      10.0\n      2022-12-19\n    \n    \n      2\n      Sun Feb 12 10:59:44 GMT+01:00 2023\n      Sun Feb 12 11:57:10 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n      57.0\n      2023-02-12\n    \n    \n      3\n      Sun Feb 12 13:58:58 GMT+01:00 2023\n      Sun Feb 12 15:58:58 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n      120.0\n      2023-02-12\n    \n    \n      4\n      Sun Feb 12 17:52:56 GMT+01:00 2023\n      Sun Feb 12 18:57:36 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n      64.0\n      2023-02-12\n    \n    \n      5\n      Sun Feb 12 20:53:34 GMT+01:00 2023\n      Sun Feb 12 22:53:34 GMT+01:00 2023\n      Study\n      NaN\n      Cedar\n      True\n      120.0\n      2023-02-12\n    \n  \n\n\n\n\n\n\nResult\n\n\nCode\ndf['Study Date'] = pd.to_datetime(df['Study Date'])\ndf.set_index('Study Date', inplace=True)\ndaily_study_time = df.resample('D')['Study Time'].sum()\n\n# Creating a list of dates from the start date to end date.\n\nstart_date = df.index.min()\nend_date = df.index.max()\ndate_range = pd.date_range(start=start_date, end=end_date, freq='D')\n\n# Create a 2D array to represent the grid of squares\nnum_weeks = int(np.ceil(len(date_range) / 7))\ngrid = np.zeros((num_weeks, 7))\n\n# Fill in the grid with study times\nfor i, date in enumerate(date_range):\n    if date in daily_study_time.index:\n        grid[i // 7, i % 7] = daily_study_time[date]\n\n# Define the color map for the squares\ncmap = ['rgb(235, 237, 240)', 'rgb(198, 228, 139)', 'rgb(123, 201, 111)', 'rgb(35, 154, 59)', 'rgb(25, 97, 39)']\nbounds = [0, 60, 120, 180, 300, np.inf]\n\n# Define data for heatmap\ndata = go.Heatmap(z=grid, colorscale=cmap, zmin=bounds[0], zmax=bounds[-1], colorbar=dict(title=\"Studied Time\"))\n\n# Define layout\nlayout = go.Layout(\n    title='Study Activity',\n    xaxis=dict(\n        title='Day of Week',\n        tickmode='array',\n        tickvals=np.arange(7),\n        ticktext=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'],\n    ),\n    yaxis=dict(\n        title='Month',\n        tickmode='array',\n        tickvals=np.arange(num_weeks),\n        ticktext=[date.strftime('%b %Y') for date in date_range[::7]],\n    ),\n)\n\n# Create and show figure\nfig = go.Figure(data=data, layout=layout)\n\n\n\n\nCode\nfig_b = go.Figure(data=[go.Bar(x=df['Study Time'].index, y=df['Study Time'].values ,marker = dict(color = \"#8cc914\"))])\nfig_b.update_layout(xaxis_title=\"Study Days\", yaxis_title=\"Study Time\")\n\n# add range selector\nfig_b.update_layout(xaxis=dict(rangeselector=dict(\n    buttons=list([\n        dict(count=7, label=\"1w\", step=\"day\", stepmode=\"backward\"),\n        dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n        dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n        dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n        dict(count=1, label=\"TEST\", step=\"day\", stepmode=\"todate\"),\n        dict(step=\"all\")\n    ])\n), rangeslider=dict(visible=True), type=\"date\"))\n\n\n\n\n\n                                                \n\n\n\n                                                \n\n\nThats all for now 👋"
  },
  {
    "objectID": "Project_index/fastai_index.html",
    "href": "Project_index/fastai_index.html",
    "title": "Page",
    "section": "",
    "text": "fast.ai Practical Deep Learning for Coders 2022 Notes\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\np2-02_meanshift\n\n\n\n\n\n\n\nClustering\n\n\nfastai\n\n\n\n\nCreating meanshift algorithm + visualization of the algorithm.Homework, k_means_clustering algorithm.\n\n\n\n\n\n\nMar 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np2-01 Matrix Multiplication From Foundation.\n\n\n\n\n\n\n\nfastai\n\n\nFoundations\n\n\n\n\nCreating matrix multiplication from foundation me. With just python\n\n\n\n\n\n\nMar 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\np1-08 Collaborative Filtering Deep Dive\n\n\n\n\n\n\n\nfastai\n\n\nTabularData\n\n\n\n\nBuilding Collaborative Filter on MovieLens dataset.\n\n\n\n\n\n\nMar 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1-07 How random forests really work\n\n\n\n\n\n\n\nML\n\n\nRandom Forest\n\n\nfastai\n\n\n\n\nBuilding Random Forests from scratch.\n\n\n\n\n\n\nMar 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1-05 Linear model and Neural Net from scratch\n\n\n\n\n\n\n\nfastai\n\n\nDeepLearning\n\n\n\n\nCreating a linear model and Neural Net just with pytorch.tensors\n\n\n\n\n\n\nMar 17, 2023\n\n\n\n\n\n\n  \n\n\n\n\np1-04 How does a neural net really work\n\n\n\n\n\n\n\nfastai\n\n\n\n\nFitting functions and activations functions.\n\n\n\n\n\n\nMar 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1-02 Saving a basic fastai model\n\n\n\n\n\n\n\nfastai\n\n\n\n\nCreating, training and saving a basic model using fastai liblary\n\n\n\n\n\n\nMar 17, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Page",
    "section": "",
    "text": "Hello Welcome to my Page 👋\nHello,I am aspiring Data Scientist and Software Enginer,I am currently in my second year Computer Science, currently exploring the fascinating field of Deep Learning.\nAlthough I may not have any commercial experience, I have been passionately delving into the intricacies of Deep Learning concepts and have created some small projects to demonstrate my skills. You can find most of my projects on my Github and Kaggle profile, or by visiting the Projects tab on this page, or by clicking the button bellow.\nMy projects\n\n\nContact Information"
  }
]